参考书籍：
- [《机器学习高级实践·计算广告、供需预测、智能营销、动态定价》机械工业出版社](https://book.douban.com/subject/36648547/)


---

# 前言

算是又开了个坑吧，之前写过B站的Recommendation System，但是理论很美好，实践出大问题。最严重的问题是正负类比例严重失衡，而且SMOTE没有丝毫作用，所以打算先学习他人的处理方法。

本文分为小型demo与项目实践两部分。
小型demo可能是对于某些数据集的解决方案，而项目实践则是对于某个项目的解决方案。比如kaggle上的好例子可能归为小型demo，而比较大的项目或者是某一类数据的分析方法则归为项目实践。

如果是他人的项目，会使用@引用。如果是自己的项目，会使用©。

---

# 1. 小型demo


---

# 2. 项目实践

## 2.1 计算广告——广告点击率预估 @《机器学习高级实践·计算广告、供需预测、智能营销、动态定价》

| 代码 | 描述 |
| --- | --- |
| [data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) | 数据预处理 |
| [data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) | 数据可视化 |

### 2.1.1 项目背景

#### 2.1.1.1 计算广告的目标

广告主(Demand,需求方)期望在线广告针对性更强，广告平台(Supply,供给方)期望广告点击率更高，用户期望广告更加个性化。
- **针对性**：广告能依据用户偏好精准地投放给潜在的、有需求的用户，以提高投入产出比。
- **点击率**：是广告投放效果的重要指标，是广告点击次数与广告曝光次数的比值。
- **个性化**：是指广告内容、形式、投放时间等因素能够根据用户的个性化需求进行定制。

#### 2.1.1.2 计算广告的术语

Computational Advertising：利用计算机技术、数学模型、统计方法等手段，通过对广告投放对象、广告内容、广告投放时机等进行精准分析，实现广告投放效果的最大化。

| 术语 | 全称 | 含义 |
| --- | --- | --- |
| **CTR** | Click-Through Rate, 点击率 | 点击次数÷曝光次数 |
| **CVR** | Conversion Rate, 转化率 | 转化次数÷点击次数，用户点击广告后完成特定行为Action(购买、下载等)的比例 |
| CPM | Cost Per Mille, 千次曝光成本 | 每千次曝光需支付的费用 |
| CPC | Cost Per Click, 单次点击成本 | 每次点击需支付的费用 |
| CPA | Cost Per Action, 单次行为成本 | 每次有效行为转化需支付的费用 |
| CPT | Cost Per Time, 单次时间成本 | 按播放时长计费 |
| **ROI** | Return On Investment, 投资回报率 | 收益÷成本 |

#### 2.1.1.3 计算广告的流程

<p style="color:#EC407A; font-weight:bold">1. 合约广告</p>

```mermaid
graph LR
    A[广告主与媒体签订合约] --> B[确定广告展示位置和时间]
    B --> C[广告上线与展示]
    C --> D[监控广告效果]
    D --> E[合约结束]
```

合约广告是单次交易，但粗粒度的广告投放方式会导致成本、收益不可控，不够理性。核心问题是：
1. 构建受众标签：聚类、分类、关联规则挖掘等
2. 事前流量预测：时序模型，如ARIMA、Prophet、LSTM、Transformer、DeepAR等
3. 在线流量分配：$\max \sum_{i=1}^{n} (r_i - c_i), \text{s.t.}  \sum_{i=1}^{n} d_i \leq D$，其中$r_i$是收入，$c_i$是成本，$d_i$是投放量，$D$是需求方的总投放量。转为为优化问题。

<p style="color:#EC407A; font-weight:bold">2. 竞价广告</p>

ADX(Ad Exchange)：广告交易平台，负责广告位的竞价、广告投放、广告效果监控等。
DSP(Demand Side Platform)：需求方平台。

```mermaid
graph LR
    A[用户访问网站] --> B[ADX发起广告位竞价并传输数据给DSP]
    B --> C[DSP出价]
    C --> D[ADX对DSP出价进行排序]
    D --> E[最高价的DSP获得广告位]
    E --> F[广告展示]
```

竞价广告是实时交易，是精细化的广告投放方式，但是需要解决的问题更多，尤其是CTR预估与实时性。

### 2.1.2 核心算法

```mermaid
graph LR
    A[广告库存] --> B[规则初筛]
    B --> C[粗排]
    C --> D[精排]
    D --> E[重排]
```

该部分内容在推荐系统中有详细介绍，这里不再赘述。

### 2.1.3 数据集介绍

[Ali_Display_Ad_Click](https://tianchi.aliyun.com/dataset/56)是阿里巴巴提供的一个淘宝展示广告点击率预估数据集。114万用户8天内的广告展示/点击日志（2600万条记录），用前面7天的做训练样本（20170506-20170512），用第8天的做测试样本（20170513）。
目前已有的研究：[CSDN](https://blog.csdn.net/weixin_39802763/article/details/105766042)，[CSDN](https://blog.csdn.net/sinat_28015305/article/details/108065830)，[arXiv](https://arxiv.org/abs/1706.0697)，[arXiv](https://arxiv.org/abs/1704.05194)。

1. **ad_feature.csv 广告信息表**，29.8MB

| **属性** | adgroup_id | cate_id | campaign_id | customer | brand | price |
| --- | --- | --- | --- | --- | --- | --- |
| **解释** | 广告ID | 商品类别ID | 广告计划ID | 广告主ID | 品牌ID | 商品价格 |
| **第一行数据** | 63133 | 6406 | 83237 | 1 | 95471 | 170.0 |

- 一个广告ID对应一个商品，一个商品属于一个类目，一个商品属于一个品牌。

2. **raw_sample.csv 原始样本骨架**，用户-广告展示/点击数据，1.01GB

| **属性** | user | time_stamp | adgroup_id | pid | nonclk | clk |
| --- | --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 日志时间戳 | 广告ID | 广告资源位 | 未点击 | 点击 |
| **第一行数据** | 581738 | 1494137644 | 1 | 430548_1007 | 1 | 0 |

- 未点击的时候，clk=0，nonclk=1

3. **user_profile.csv 用户信息表**，22.9MB

| **属性** | userid | cms_segid | cms_group_id | final_gender_code | age_level | pvalue_level | shopping_level | occupation | new_user_class_level |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 微群ID | 微群ID | 性别 | 年龄分层 | 消费能力 | 购物深度 | 职业 | 城市层级 |
| **第一行数据** | 234 | 0 | 5 | 2 | 5 |  | 3 | 0 | 3 |

- 性别：1-男，2-女
- 消费能力：1-低，2-中，3-高
- 购物深度：1-低，2-中，3-高
- 职业：是否是大学生，0-否，1-是

4. **behavior_log.csv 用户行为日志表**，22GB

| **属性** | user | time_stamp | btag | cate | brand |
| --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 日志时间戳 | 行为类型 | 商品类别ID | 品牌ID |
| **第一行数据** | 558157 | 1493741625 | pv | 6250 | 91286 |

- 行为标签：pv-浏览，cart-加入购物车，fav-喜欢，buy-购买

5. **基线AUC：0.622**


### 2.1.4 数据预处理与分析

#### 2.1.4.1 读取数据

源代码请查看[data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) 。

因为数据过大，使用采样读取`behavior_log.csv`，并保留采样的用户，主要代码如下：

```python
n_sample = int(frac * total_rows)  # 采样的行数
behavior_log = pd.read_csv(f'{root_path}/behavior_log.csv', nrows=n_sample)
sampled_users = behavior_log['user'].unique()  # 采样的用户
raw_sample = raw_sample[raw_sample['user'].isin(sampled_users)]  # 保留采样的用户
user_profile = user_profile[user_profile['userid'].isin(sampled_users)]
```

假设选取0.1%(即使0.1%也很大了)的`behavior_log.csv`数据，采样其他表格，最后得到的数据shape为：
```python
user_profile用户数据： (185915, 9)
raw_sample样本数据： (7114606, 6)
behavior_log用户行为数据： (723268, 5)
ad_feature广告特征数据： (846811, 6)
```

顺便，对同一含义的不同列名进行统一，统一为`user_id`和`cate_id`。


#### 2.1.4.2 缺失值&编码

`pvalue_level, new_user_class_level`、 `brand`有缺失值，比例分别为52.70%,26.45%、29.09%。在初步分析阶段暂时不做缺失值处理。

~~对`cate_id`, `brand`进行编码，使用`LabelEncoder`。意义不大，故不做了~~

#### 2.1.4.3 了解特征

源代码请看[data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) 。

下面列举部分特征的取值情况（分布较为均匀的、特征是ID的不列出，部分数据使用给出图像方便查看）：

1. **ad_feature**

基本为ID型数据，略去。

2. **raw_sample**

pid(2个取值)：

| 430548_1007 | 430539_1007 |
| --- | --- |
| 4016881 | 3097725 |

clk(2个取值)：

| 0 | 1 |
| --- | --- |
| 6722820 | 391786 |

样本不平衡率$\frac{6722820}{391786} = 17.16$，严重不平衡。$0$类占比94.5%。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/raw-sample-hour-distribution.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">用户在傍晚的数据反而较少，有可能是数据集做的时间脱敏</p>
    </div>
</div>

3. **user_profile**

final_gender_code(2个取值)：

| 1 | 2 |
| --- | --- |
| 52258 | 133657 |

女性是男性的2.56倍，占71.9%。

pvalue_level消费能力(3个取值)：

| 1.0 | 2.0 | 3.0 |
| --- | --- | --- |
| 24351 | 55492 | 8467 |

大部分用户消费能力在中等水平与偏下水平。

shopping_level购物深度(3个取值)：

| 1 | 2 | 3 |
| --- | --- | --- |
| 7318 | 16606 | 161991 |

大部分用户购物深度在高水平。

occupation是否大学生(2个取值)：

| 0 | 1 |
| --- | --- |
| 174450 | 11465 |

大学生只占6.2%。

new_user_class_level城市层级(4个取值)：

| 1.0 | 2.0 | 3.0 | 4.0 |
| --- | --- | --- | --- |
| 16012 | 63962 | 33769 | 25057 |

大部分用户城市层级在中等水平与偏下水平。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/user_profile-distribution-1.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">分析见上文</p>
    </div>
</div>

4. **behavior_log**

btag(4个取值)：

| pv | cart | buy | fav |
| --- | --- | --- | --- |
| 688331 | 16119 | 9577 | 9241 |

加入购物车的比例是$\frac{16119}{688331} = 2.34\%$，购买的比例是$\frac{9577}{688331} = 1.39\%$。buy与fav相近，可能是用户购买之后加入了收藏，（yysy我才知道淘宝有收藏功能）具体的行为链还需要进一步分析。


day（20个取值，下面只列出最主要的3个）：

| 3 | 2 | 1 |
| --- | --- | --- |
| 641372 | 81573 | 190 |

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/day-hour-distribution-1.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">用户在傍晚的日志反而较少，有可能是数据集做的时间脱敏</p>
    </div>
</div>


#### 2.1.4.4 异常值等处理

让我们转回[data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) 。

通过2.1.4.3的图像输出发现ad_feature的price有极高的异常值(99999999.0)，采取`ad_feature.loc[ad_feature['price'] > 999999, 'price'] = 999999`处理。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/ad_feature-price-distribution-90percent.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">使用price_90 = np.percentile(ad_feature["price"], 90), sns.histplot(ad_feature["price"][ad_feature["price"] <= price_90], kde=False, bins=100)绘制前90%的数据</p>
    </div>
</div>

对pid进行编码。`raw_sample['pid'] = raw_sample['pid'].apply(lambda x: 1 if x == '430548_1007' else 2)`

#### 2.1.4.5 corr

回到[data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) 。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/corr.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;"></p>
    </div>
</div>

后三个数据的corr较小，我们这里重点分析**用户画像**。
首先是`cms_segid`与`cms_group_id`与性别、年龄都有一定的相关性，可能是相似的用户往往会聚在一起，而这种聚集与购物深度、职业、城市层级等特征关系很小。
现在不考虑userid、cms_segid、cms_group_id这三列，`age_level`与`pvakue_level`相关性为0.25，与`occupation`相关性为-0.29，年龄越大消费能力越高，越不可能为大学生。`pvalue_level`与`occupation`和`new_user_class_level`相关性都是-0.11，说明消费能力越高越不可能为大学生，越可能是一线城市。


#### 2.1.4.6 合并数据

```python
# 1. 合并 raw_sample 和 user_profile。通过主键user_id 字段进行连接。
merged_data = pd.merge(raw_sample, user_profile, on='user_id', how='left')
# 2. 再将上述结果与 ad_feature 合并。通过主键adgroup_id 字段进行连接。
ad_u_data = pd.merge(merged_data, ad_feature, on='adgroup_id', how='left')
```
保存为`ad_u_data.csv`。
其实这等价于
```SQL
SELECT * FROM raw_sample
LEFT JOIN user_profile ON raw_sample.user_id = user_profile.userid
LEFT JOIN ad_feature ON raw_sample.adgroup_id = ad_feature.adgroup_id
```


#### 2.1.4.7 其余可视化

点击量/点击率的时间趋势：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/clk_nonclk_trend.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">点击与不点击的数量随时间的变化趋势，十分有规律</p>
    </div>
</div>


<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/clk_rate_trend.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">点击率的变化不大</p>
    </div>
</div>

二八定律：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/28定律-brand.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">小于20%的品牌贡献了大于80%的广告数量</p>
    </div>
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/28定律-userclk.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">约20%的用户贡献了80%的点击量</p>
    </div>
</div>

性别、价格与点击率：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/price-density-by-gender.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">男性点击的商品价格一般比女性高，并且不点击的商品价格均值也都比点击的商品要高一点点</p>
    </div>
</div>


### 2.1.4.8 构建embedding向量

代码可见[P_AD_3_book_optimbyGPT](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_3_book_optimbyGPT.py)，这是书上的代码经过GPT优化后的版本。
这个代码依据id进行embedding来训练(使用点积结果与clk的BCE进行训练)，书上最后给了acc评价指标(我还加上了AUC指标)，但是我觉得这样评估不合适，如果是使用id进行embedding，那么说明valid的时候会被train的id泄露，不然怎么acc那么高呢？
模型输出如下：
```python
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 adgroup_id (InputLayer)     [(None, 1)]                  0         []                            
                                                                                                  
 user_id (InputLayer)        [(None, 1)]                  0         []                            
                                                                                                  
 ad_embedding (Embedding)    (None, 1, 50)                3842300   ['adgroup_id[0][0]']          
                                                                                                  
 user_embedding (Embedding)  (None, 1, 50)                5821500   ['user_id[0][0]']             
                                                                                                  
 dot_product (Dot)           (None, 1, 1)                 0         ['ad_embedding[0][0]',        
                                                                     'user_embedding[0][0]']      
                                                                                                  
 reshape (Reshape)           (None, 1)                    0         ['dot_product[0][0]']         
                                                                                                  
 dense (Dense)               (None, 1)                    2         ['reshape[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9663802 (36.86 MB)
Trainable params: 9663802 (36.86 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
Epoch 1/3
257/257 [==============================] - 25s 95ms/step - loss: 0.6433 - accuracy: 0.7200 - val_loss: 0.4917 - val_accuracy: 0.9755
Epoch 2/3
257/257 [==============================] - 24s 95ms/step - loss: 0.4434 - accuracy: 0.9708 - val_loss: 0.3251 - val_accuracy: 0.9906
Epoch 3/3
257/257 [==============================] - 22s 88ms/step - loss: 0.2982 - accuracy: 0.9870 - val_loss: 0.2243 - val_accuracy: 0.9949
Predicted 0: 100308, Predicted 1: 4645
Predicted 0 ratio: 0.96, Predicted 1 ratio: 0.04
Validation AUC: 0.9999
              precision    recall  f1-score   support

           0       0.99      1.00      1.00     99772
           1       1.00      0.90      0.95      5181

    accuracy                           0.99    104953
   macro avg       1.00      0.95      0.97    104953
weighted avg       0.99      0.99      0.99    104953
```
我认为valid一开始就这么高是因为id泄露了，所以这个模型是不可靠的，只是一个embedding的示例。
另外暂时无法使用pytorch复现结果，原因未知。
