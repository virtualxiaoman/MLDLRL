# $\color{ee0000} 1.\ 简介 $
## $\color{66ccff} 1.1\ 技术架构 $
$$ f(用户信息，资源信息，上下文信息，用户行为序列) = 推荐结果 $$

## $\color{66ccff} 1.2\ 搜广推 $
- 搜索：围绕着搜索词的信息高效获取问题
- 广告：直接增加公司收入
- 推荐：提高用户留存和活跃度
[具体区别-知乎-王喆 ↗​](https://zhuanlan.zhihu.com/p/430431149)

## $\color{66ccff} 1.3 \ 架构 $
### $\color{39c5bb} 1.3.1\ 系统架构 $
1. **离线层**：不用实时数据，不提供实时响应；(大量)
可以每天更新一次。
2. **近线层**：使用实时数据，不保证实时响应；(几分钟)
可以在用户访问时更新。
3. **在线层**：使用实时数据，保证实时在线服务。(几十毫秒)
在用户访问时实时响应，如开屏时的推荐。

<div style="text-align: center;">
    <img src="assets/推荐系统/images/image.png" alt="描述文本" width="300"/>
    <img src="assets/推荐系统/images/image-1.png" alt="描述文本" width="500"/>
</div>



### $\color{39c5bb} 1.3.2 \ 算法架构 $

<div style="text-align: center;">
    <img src="assets/推荐系统/images/image-3.png" alt="描述文本" width="500"/>
</div>

- **召回**：不需要十分准确，但不可遗漏。快速稳定，兴趣多元、内容多样。
  - 非个性化召回、个性化召回
    - 个性化召回：content-based、behavior-based、feature-based
- **粗排**：兼顾精准性和低延迟。
- **精排**：精准性优先，目标ctr、cvr(点击率、转化率)，指标AUC。
  - 样本、特征、模型。
- **重排**：多样性优先，目标Point Wise、Pair Wise、List Wise(点对、对对、列表)，指标NDCG。
  - 基于运营策略、多样性、context上下文。

<details>

<summary><span style="color:#009688; font-weight:bold">点击展开具体算法</span></summary>

<p style="color:#EC407A; font-weight:bold">1. 画像层</p>

- 文本理解
  - RNN、TextCNN、FastText、Bert
- 关键词标签
  - TF-IDF、Bert、LSTM-CR
- 内容理解
  - TSN、RetinaFace、PSENet

<p style="color:#EC407A; font-weight:bold">2. 召回/粗排</p>

- 经典模型召回
  对user和item分别打上Embedding，然后user与item在线进行KNN计算实时查询最近领结果作为召回结果
  - FM、双塔DSSM、Multi-View DNN
- 序列模型召回
  有监督Next Item Prediction，无监督Sum Pooling
  - CBOW、Skip-Gram、GRU、Bert
- 用户序列拆分
  把用户行为序列打到多个embedding上(类似聚类)
  - Multi-Interest Network with Dynamic Routing for Recommendation at Tmall
- 知识图谱
  可解释性好但是效果差
  - KGAT、RippleNet
- 图模型
  编码的是静态知识，而不是用户比较直接的行为数据，所以效果一般
  - GraphSAGE、PinSage

<p style="color:#EC407A; font-weight:bold">3. 精排</p>

- 特征交叉模型
  - DCN、DeepFM、xDeepFM
- 序列模型
  关注用户此刻的兴趣向量（user interest vector）
  - DIN、DSIN、DIEN、SIM
- 多模态信息融合
  - Image Matters: Visually modeling user behaviors using Advanced Model Server、UMPR
- 多任务模型
  点击率模型(标题党)、时长模型(长视频)、完播率模型(短视频)
  - ESSM、MMoE、DUP
- 强化学习
  - DQN、Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology
- 跨域推荐
  比如QQ音乐的信息如何使用在腾讯视频上
  - DTCDR、MV-DNN、EMCDR
</details>

## $\color{66ccff} 1.4\ 参考资料 $

[电子书-FunRec ↗](https://datawhalechina.github.io/fun-rec/#/)
[纸质书-推荐系统技术原理与实践 ↗](https://www.epubit.com/bookDetails?id=UB831721e9d193a)


# $\color{ee0000} 2.\ 召回 $

<div style="text-align: center;">
    <img src="assets/推荐系统/images/image-4.png" alt="描述文本" width="500"/>
    <p>多路召回</p>
    <img src="assets/推荐系统/images/image-5.png" alt="描述文本" width="500"/>
    <p>召回技术的演进图谱</p>
</div>

## $\color{66ccff} 2.1\ 基于协同过滤的召回 $

### $\color{39c5bb} 2.1.1\ 协同过滤(CF) $
<span style="color:#EC407A; font-weight:bold">协同过滤</span>(Collaborative Filtering)：基于用户行为数据，找到相似用户或物品， 重点是计算用户/物品之间的相似度。
- **基于用户的协同过滤**(UserCF)：找到相似用户，根据相似用户的行为推荐物品。
- **基于物品的协同过滤**(ItemCF)：找到相似物品，根据该用户的历史行为推荐物品。

### $\color{39c5bb} 2.1.2\ 相似度计算 $
记用户为$u,v$，物品为$i,j$，用户集合为$U$。

<p style="color:#EC407A; font-weight:bold">1. Jaccard相似度</p>

- 反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例，适用于隐式反馈的二值数据。
  - **隐式反馈数据**是指从用户行为中隐含地推断出的偏好数据，而不是用户明确表达的偏好或评分。这种类型的数据通常来源于用户在系统中的交互行为，例如点击、浏览、购买等。这些行为可以隐含地表示用户对这些物品有某种程度的兴趣或偏好，但具体的兴趣程度并不明确。隐式反馈数据只记录用户是否与某个物品有过交互(0,1)，而不关心交互的具体程度。
  $$sim_{uv}=\frac{|N(u)\cap N(v)|}{|N(u)|\cup|N(v)|}$$
- 其中，$N(u)$,$N(v)$ 分别表示用户$u$和用户$v$交互物品的集合。
- 由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户**是否**会对某物品进行打分， 而不是预估用户会对某物品打多少分。
- `from sklearn.metrics.pairwise import jaccard_similarity_score`

<p style="color:#EC407A; font-weight:bold">2. 余弦相似度</p>

- 衡量了两个向量的夹角，适用于度量向量之间的**相似度**。
- 用户相似度
  $$sim_{uv}=\frac{|N(u)\cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}$$
- 物品相似度
  $$sim_{ij}=\frac{|I(i)\cap I(j)|}{\sqrt{|I(i)|\cdot|I(j)|}}$$
- 其中，$N(u)$,$N(v)$ 分别表示用户$u$和用户$v$交互物品的集合，$I(i)$,$I(j)$ 分别表示物品$i$和物品$j$被用户交互的集合。
- 设矩阵$A$为用户-物品交互矩阵，$A_{ij}=0,1$表示用户$i$对物品$j$是否交互，$A^{(i)}$表示用户$i$的评分向量，$A_j$表示物品$j$的评分向量，用户之间的相似度计算公式为：$sim_{uv}=\frac{A^{(u)}\cdot A^{(v)}}{\|A^{(u)}\|\cdot\|A^{(v)}\|}=\frac{u\cdot v}{|u|\cdot|v|}=cos(u,v)$。为了节省内存，$A$常使用字典进行存储。
- `from sklearn.metrics.pairwise import cosine_similarity`

<p style="color:#EC407A; font-weight:bold">3. Pearson相似度</p>

- 衡量了两个向量之间的线性相关性，度量的是两个变量的**变化趋势**是否一致，两个随机变量是不是同增同减。因为先对向量进行中心化处理，所以可以消除用户评分的绝对值差异，适用于评分数据，不适合布尔值向量。
- 用户之间的余弦相似度计算：$sim_{uv}= \frac{r_u\cdot r_v}{|r_u|\cdot |r_v|} = \frac{\sum_ir_{ui}*r_{vi}}{\sqrt{\sum_ir_{ui}^2}\sqrt{\sum_ir_{vi}^2}}$
- 皮尔逊相关系数与余弦相似度的计算公式非常类似：
$$sim(u,v)=\frac{\sum_{i}(r_{ui}-\bar{r}_u)(r_{vi}-\bar{r}_v)}{\sqrt{\sum_{i}(r_{ui}-\bar{r}_u)^2}\sqrt{\sum_{i}(r_{vi}-\bar{r}_v)^2}}\in[-1,1]$$
- 其中，$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对物品$i$是否有交互(或具体评分值); $\bar{r}_u,\bar{r}_v$分别表示用户$u$和用户$v$交互的所有物品交互数量(或评分的平均值)；
- `from scipy.stats import pearsonr`, `np.corrcoef()`

<p style="color:#EC407A; font-weight:bold">4. Tanimoto</p>

$$sim_{uv}= \frac{r_u\cdot r_v}{\left \| r_u \right \| ^2+\left \| r_v \right \| ^2-{r_u\cdot r_v}}$$

### $\color{39c5bb} 2.1.3\ 基于用户的协同过滤(UserCF) $

<p style="color:#EC407A; font-weight:bold">1. 计算方法</p>

根据已有的用户向量计算用户$y$与其他用户$X$的相似度，得到与用户$y$最相似的$n$个用户。根据这 $n$个用户对物品$k$的评分情况和与$y$的相似程度猜测出$y$对$k$的评分。如果评分比较高的话， 就把$k$推荐给$y$， 否则不推荐。


| 用户 | 物品$I_1$ | 物品$I_2$ | 物品$I_3$ | ··· | 物品$k$ |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 用户$y$ | 1 | 0 | 1 | ... | ? |
| 用户$X_1$ | 0 | 1 | 1 | ... | 0 |
| 用户$X_2$ | 1 | 0 | 0 | ... | 1 |



- **计算相似度**：计算用户$y$与其他用户$X$的相似度。可以采用余弦相似度、皮尔逊相似度等，找到与用户$y$最相似的$n$个用户。
- **推荐物品**：根据这$n$个用户对物品$k$的评分情况和与$y$的相似程度猜测出$y$对$k$的评分。公式如下：
$$r_{y,k}=\frac{\sum_{X_i\in N(y)}sim(y,X_i)\cdot r_{X_i,k}}{\sum_{X_i\in N(y)}|sim(y,X_i)|}$$
  - 式中，$r_{y,k}$表示用户$y$对物品$k$的评分，$N(y)$表示与用户$y$最相似的$n$个用户，$sim(y,X_i)$表示用户$y$与用户$X_i$的相似度，$r_{X_i,k}$表示用户$X_i$对物品$k$的评分。
  - 考虑到用户的评分主观性强，存在偏置(评分习惯)，有的用户喜欢打高分， 有的用户喜欢打低分的情况。因此公式变为：
$$r_{y,k}=\bar{r}_y+\frac{\sum_{X_i\in N(y)}sim(y,X_i)\cdot (r_{X_i,k}-\bar{r}_{X_i})}{\sum_{X_i\in N(y)}|sim(y,X_i)|}$$
  - 式中，$\bar{r}_y$表示用户$y$的平均评分，$\bar{r}_{X_i}$表示用户$X_i$的平均评分。因此消除了用户的评分偏置。
- **推荐结果**：实际计算时不止一个物品$k$，而是所有用户未交互过的物品，所以就根据评分排序，取TopN作为推荐结果。

<p style="color:#EC407A; font-weight:bold">2. 代码实现</p>
<details>
<summary><span style="color:#009688; font-weight:bold">点击展开查看完整代码</span></summary>

```python
import numpy as np
import pandas as pd

# 用户-物品评分数据，因为是稀疏矩阵，所以使用字典存储，后面也使用字典来计算用户相似度
user_data = {'y': {'A': 5, 'B': 3, 'C': 4, 'D': 4},
             'X1': {'A': 3, 'B': 1, 'C': 2, 'D': 3, 'k': 3},
             'X2': {'A': 4, 'B': 3, 'C': 4, 'D': 3, 'k': 5},
             'X3': {'A': 3, 'B': 3, 'C': 1, 'D': 5, 'k': 4},
             'X4': {'A': 1, 'B': 5, 'C': 5, 'D': 2, 'k': 1}
             }
user_names = list(user_data.keys())
# 用户相似度矩阵，行列均为用户名称
similarity_matrix = pd.DataFrame(
    np.identity(len(user_data)),
    index=user_names,
    columns=user_names
)

# 遍历每条用户-物品评分数据
for u1, info1 in user_data.items():
    for u2, info2 in user_data.items():
        if u1 == u2:
            continue
        vec1, vec2 = [], []  # 用户u1, u2对应的评分向量
        for item1, rating1 in info1.items():
            rating2 = info2.get(item1, -1)
            if rating2 == -1:
                continue
            vec1.append(rating1)
            vec2.append(rating2)
        similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[0][1]  # 计算不同用户之间的皮尔逊相关系数
print(similarity_matrix)

target_user = 'y'
topN = 2
sim_users = similarity_matrix[target_user].sort_values(ascending=False)[1:topN+1].index.tolist()  # 去除自身
print(f'与用户{target_user}最相似的{topN}个用户为：{sim_users}')

weighted_scores = 0.
corr_values_sum = 0.
target_item = 'k'
for user in sim_users:
    sim_y_Xi = similarity_matrix[target_user][user]
    Xi_mean_rating = np.mean(list(user_data[user].values()))  # 用户Xi的平均评分
    weighted_scores += sim_y_Xi * (user_data[user][target_item] - Xi_mean_rating)  # 分子
    corr_values_sum += sim_y_Xi  # 分母
y_mean_rating = np.mean(list(user_data[target_user].values()))
target_item_pred = y_mean_rating + weighted_scores / corr_values_sum  # 预测用户y对物品k的评分
print(f'用户{target_user}对物品{target_item}的预测评分为：{target_item_pred}')
```
输出：
```
           y        X1        X2        X3        X4
y   1.000000  0.852803  0.707107  0.000000 -0.792118
X1  0.852803  1.000000  0.467707  0.489956 -0.900149
X2  0.707107  0.467707  1.000000 -0.161165 -0.466569
X3  0.000000  0.489956 -0.161165  1.000000 -0.641503
X4 -0.792118 -0.900149 -0.466569 -0.641503  1.000000
与用户y最相似的2个用户为：['X1', 'X2']
用户y对物品k的预测评分为：4.871979899370592
```
</details>


<p style="color:#EC407A; font-weight:bold">3. 优缺点</p>

- **优点**：
  - 简单、易于实现，不需要物品的内容信息，只需要用户的行为数据。
- **缺点**：
  - 商品量大，不同用户之间买的物品重叠性较低，导致相似度计算困难。尤其不适合冷启动、低频购买次数的问题。
  - 用户量大，用户相似度矩阵的存储开销非常大。


<p style="color:#EC407A; font-weight:bold">4. 算法评估</p>

UserCF和ItemCF结果评估是一致的。

- **召回率**：召回率越高，说明推荐系统越能将用户喜欢的物品推荐给用户。
  $$Recall=\frac{推荐成功的物品数}{用户喜欢的物品数}=\frac{推荐的topN\cap 用户在测试集上喜欢的物品集合}{用户在测试集上喜欢的物品集合}$$
- **准确率**：准确率越高，说明推荐系统推荐的物品越能让用户喜欢。
  $$Precision=\frac{推荐成功的物品数}{推荐出来的物品数}=\frac{推荐的topN\cap 用户在测试集上喜欢的物品集合}{推荐的topN}$$
- **覆盖率**：覆盖率反映了推荐算法发掘长尾的能力。覆盖率越高，说明推荐系统能够将长尾物品推荐给用户。
  $$Coverage=\frac{\bigcup_{u\in U}给用户u推荐的topN}{总物品数}=\frac{set(推荐系统给所有用户推荐的商品)}{总物品数}$$
- **多样性**：多样性是指推荐系统推荐出来的物品之间的差异性。多样性越高，说明推荐系统推荐的物品之间的差异性越大。
  $$Diversity=\frac{1}{|U|}\sum_{u\in U}diversity(u)$$
  - 其中，$diversity(u)$表示用户$u$推荐列表中物品之间的差异性：$diversity(u)=\frac2{|I_u|(|I_u|-1)}\sum_{i,j\in I_u,i\neq j}\mathrm{sim}(i,j)$，其中，$I_u$ 是用户 $u$ 的推荐列表，$|I_u|$ 是推荐列表中物品的数量，$\text{sim}(i, j)$ 是物品 $i$ 和物品 $j$ 之间的相似度。
- **新颖度**：用推荐列表中物品的平均流行度度量推荐结果的新颖度。如果推荐出的物品都很热门，说明推荐的新颖度较低。由于物品的流行度分布呈长尾分布，所以为了流行度的平均值更加稳定，在计算平均流行度时对每个物品的流行度取对数。
$$Novelty=\frac{1}{|I_u|}\sum_{i\in I_u}\log(p_i)$$
  - 其中，$p_i$为第$i$个物品的流行度(比如点击次数、购买次数等)。Novelty还有其他的度量方法，如$Novelty(u)=\frac{\sum_{i\in I_u}(1-Knows(u,i))}{N}$，其中Knows(u,i)是二值函数，当用户$u$了解物品$i$时取1。



### $\color{39c5bb} 2.1.4\ 基于物品的协同过滤(ItemCF) $

<p style="color:#EC407A; font-weight:bold">1. 计算方法</p>

ItemCF算法并不利用物品的内容属性计算物品之间的相似度，而是通过分析用户的行为记录来计算物品之间的相似度。该算法认为，物品$i$和物品$j$具有很大的相似度是因为喜欢$i$的用户也可能喜欢$j$。

| 用户 | 物品$I_1$ | 物品$I_2$ | 物品$I_3$ | ··· | 物品$k$ |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 用户$y$ | 1 | 0 | 1 | ... | ? |
| 用户$X_1$ | 0 | 1 | 1 | ... | 0 |
| 用户$X_2$ | 1 | 0 | 0 | ... | 1 |

- **计算相似度**：计算物品$k$与其他物品$I$之间的相似度，找出n个物品。
- **推荐物品**：同样考虑物品之间的偏置：
$$P_{y,k}=\bar{r}_k+\frac{\sum_{I_i\in N(y)}sim(k,I_i)\cdot (r_{y,I_i}-\bar{r}_{I_i})}{\sum_{I_i\in N(y)}|sim(k,I_i)|}$$
- **推荐结果**：根据评分排序，取TopN作为推荐结果。


<p style="color:#EC407A; font-weight:bold">2. 代码实现</p>
<details>
<summary><span style="color:#009688; font-weight:bold">点击展开查看完整代码</span></summary>

```python
import numpy as np
import pandas as pd

# 物品-用户评分数据
item_data = {'A': {'y': 5.0, 'X1': 3.0, 'X2': 4.0, 'X3': 3.0, 'X4': 1.0},
             'B': {'y': 3.0, 'X1': 1.0, 'X2': 3.0, 'X3': 3.0, 'X4': 5.0},
             'C': {'y': 4.0, 'X1': 2.0, 'X2': 4.0, 'X3': 1.0, 'X4': 5.0},
             'D': {'y': 4.0, 'X1': 3.0, 'X2': 3.0, 'X3': 5.0, 'X4': 2.0},
             'E': {'X1': 3.0, 'X2': 5.0, 'X3': 4.0, 'X4': 1.0}
             }
item_names = list(item_data.keys())
# 物品相似度矩阵，行列均为物品名称
similarity_matrix = pd.DataFrame(
    np.identity(len(item_data)),
    index=item_names,
    columns=item_names
)

# 遍历每条物品-用户评分数据
for i1, info1 in item_data.items():
    for i2, info2 in item_data.items():
        if i1 == i2:
            continue
        vec1, vec2 = [], []  # 物品i1, i2对应的评分向量
        for user, rating1 in info1.items():
            rating2 = info2.get(user, -1)
            if rating2 == -1:
                continue
            vec1.append(rating1)
            vec2.append(rating2)
        similarity_matrix[i1][i2] = np.corrcoef(vec1, vec2)[0][1]  # 计算不同物品之间的皮尔逊相关系数
print(similarity_matrix)

target_user = 'y'
target_item = 'E'
topN = 2
sim_items = []
sim_items_list = similarity_matrix[target_item].sort_values(ascending=False).index.tolist()
for item in sim_items_list:
    # 如果target_user对物品item评分过，也就是排除了target_item本身
    if target_user in item_data[item]:
        sim_items.append(item)
    if len(sim_items) == topN:
        break
print(f'与物品{target_item}最相似的{topN}个物品为：{sim_items}')

k_mean_rating = np.mean(list(item_data[target_item].values()))  # 物品k的平均评分
weighted_scores = 0.
corr_values_sum = 0.
for item in sim_items:
    sim_k_Ii = similarity_matrix[target_item][item]  # 物品k与物品I_i的相似度
    Ii_mean_rating = np.mean(list(item_data[item].values()))  # 物品I_i的平均评分
    weighted_scores += sim_k_Ii * (item_data[item][target_user] - Ii_mean_rating)  # 分子
    corr_values_sum += sim_k_Ii  # 分母
target_item_pred = k_mean_rating + weighted_scores / corr_values_sum
print(f'用户{target_user}对物品{target_item}的预测评分为：{target_item_pred}')
```
输出：
```
          A         B         C         D         E
A  1.000000 -0.476731 -0.123091  0.532181  0.969458
B -0.476731  1.000000  0.645497 -0.310087 -0.478091
C -0.123091  0.645497  1.000000 -0.720577 -0.427618
D  0.532181 -0.310087 -0.720577  1.000000  0.581675
E  0.969458 -0.478091 -0.427618  0.581675  1.000000
与物品E最相似的2个物品为：['A', 'D']
用户y对物品E的预测评分为：4.6
```
</details>


<p style="color:#EC407A; font-weight:bold">3. 优缺点</p>

- **优点**：
  - 适用于物品数量少，用户数量多的场景。
- **缺点**：
  - 物品数量大，计算物品之间的相似度开销大。
  - 物品的流行度分布不均匀，导致推荐出的物品都是热门物品，缺乏多样性。


### $\color{39c5bb} 2.1.5\ Swing算法 $

<p style="color:#EC407A; font-weight:bold">1. 计算方法</p>

若用户$u$和用户$v$之间除了购买过$i$外，还购买过商品$j$，则认为两件商品是具有某种程度上的相似的(准确说是“**相关度**”或者“**互补性**”)。
为了衡量$i,j$的相似性，比较同时购买了物品$i,j$的用户$u,v$，如果这两个用户共同购买的物品越少，即这两个用户原始兴趣不相似，但仍同时购买了两个相同的物品$i,j$，则物品$i,j$的相似性越高。
$$sim(i,j)=\sum_{u\in U_i\cap U_j}\sum_{v\in U_i\cap U_j}\frac{1}{\sqrt{|I_u|}\sqrt{|I_v|}}\cdot \frac1{\alpha+|I_u\cap I_v|}, u \ne v$$
其中$U_i$ 是点击过商品$i$的用户集合，$I_u$ 是用户u点击过的商品集合，$\alpha$是平滑系数,$\frac{1}{ \sqrt {|I_u|}}$, $\frac{1}{ \sqrt {|I_v|}}$是用户权重参数，来降低活跃用户的影响。


<p style="color:#EC407A; font-weight:bold">2. 代码实现</p>
<details>
<summary><span style="color:#009688; font-weight:bold">点击展开查看完整代码</span></summary>
我的实现：

```python
import numpy as np
import pandas as pd

# 物品-用户点击数据
item_data = {
    'A': {'y': 1, 'X1': 1, 'X2': 1, 'X3': 1},
    'B': {'y': 1, 'X1': 1, 'X4': 1},
    'C': {'y': 0, 'X2': 1, 'X3': 1, 'X4': 1},
    'D': {'y': 1, 'X1': 1, 'X3': 1, 'X4': 1},
    'E': {'X2': 1, 'X3': 1, 'X4': 1}
}
item_names = list(item_data.keys())
# 物品相似度矩阵，行列均为物品名称
similarity_matrix = pd.DataFrame(
    np.zeros((len(item_data), len(item_data))),
    index=item_names,
    columns=item_names
)

# 计算物品相似度矩阵
alpha = 0.5
for i1 in item_data.keys():
    for i2 in item_data.keys():
        if i1 == i2:
            continue
        common_users = set(item_data[i1].keys()).intersection(set(item_data[i2].keys()))  # 共同点击过i1, i2的用户
        if not common_users:
            similarity_matrix[i1][i2] = 0
            continue
        sum_weights = 0
        for u in common_users:
            for v in common_users:
                if u == v:
                    continue  # 跳过同一个用户
                I_u = set(item for item, users in item_data.items() if u in users)
                I_v = set(item for item, users in item_data.items() if v in users)
                I_u_v = I_u.intersection(I_v)
                weight = 1 / (np.sqrt(len(I_u)) * np.sqrt(len(I_v)) * (alpha + len(I_u_v)))
                sum_weights += weight
        similarity_matrix[i1][i2] = sum_weights

print("物品相似度矩阵：")
print(similarity_matrix)

# 计算推荐评分
target_user = 'y'
target_item = 'E'
topN = 2

# 获取与目标物品最相似的topN物品
sim_items = []
sim_items_list = similarity_matrix[target_item].sort_values(ascending=False).index.tolist()
for item in sim_items_list:
    # 排除target_item本身
    if target_user in item_data[item]:
        sim_items.append(item)
    if len(sim_items) == topN:
        break
print(f'与物品{target_item}最相似的{topN}个物品为：{sim_items}')

k_mean_rating = np.mean([item_data[target_item][user] for user in item_data[target_item]])  # 物品k的平均评分
weighted_scores = 0.
corr_values_sum = 0.
for item in sim_items:
    sim_k_Ii = similarity_matrix[target_item][item]  # 物品k与物品I_i的相似度
    Ii_mean_rating = np.mean([item_data[item][user] for user in item_data[item]])  # 物品I_i的平均评分
    weighted_scores += sim_k_Ii * (item_data[item][target_user] - Ii_mean_rating)  # 分子
    corr_values_sum += sim_k_Ii  # 分母
target_item_pred = k_mean_rating + weighted_scores / corr_values_sum if corr_values_sum != 0 else k_mean_rating
print(f'用户{target_user}对物品{target_item}的购买预测为：{target_item_pred}')
```
输出：
```
物品相似度矩阵：
          A         B         C         D         E
A  0.000000  0.164957  0.538754  0.538754  0.164957
B  0.164957  0.000000  0.142857  0.538754  0.000000
C  0.538754  0.142857  0.000000  0.428571  0.538754
D  0.538754  0.538754  0.428571  0.000000  0.142857
E  0.164957  0.000000  0.538754  0.142857  0.000000
与物品E最相似的2个物品为：['C', 'A']
用户y对物品E的购买预测为：0.42580767318794366
```

FunRec实现（未给出相似度之后的计算）：

```python
from itertools import combinations
import pandas as pd


def load_data(train_path):
    train_data = pd.read_csv(train_path, sep="\t", engine="python", names=["userid", "itemid", "rate"])  # 提取用户交互记录数据
    train_data = train_data.drop(0)  # 第一行是列名
    train_data = train_data.astype(int)  # 转换数据类型
    print(train_data.head(3))
    return train_data


def get_uitems_iusers(train):
    u_items = dict()
    i_users = dict()
    for index, row in train.iterrows():  # 处理用户交互记录
        u_items.setdefault(row["userid"], set())  # 初始化该用户row["userid"]
        i_users.setdefault(row["itemid"], set())  # 初始化该物品
        u_items[row["userid"]].add(row["itemid"])  # 得到该用户交互过的所有item
        i_users[row["itemid"]].add(row["userid"])  # 得到该物品交互过的所有user
    print("使用的用户个数为：{}".format(len(u_items)))
    print("使用的item个数为：{}".format(len(i_users)))
    return u_items, i_users


def swing_model(u_items, i_users, alpha):
    # print([i for i in i_users.values()][:5])
    # print([i for i in u_items.values()][:5])
    item_pairs = list(combinations(i_users.keys(), 2))  # C(n,2) = (n!)/(2!(n-2)!) = (n(n-1))/2 = 50*49/2 = 1225
    print("item pairs length：{}".format(len(item_pairs)))
    item_sim_dict = dict()
    for (i, j) in item_pairs:
        user_pairs = list(combinations(i_users[i] & i_users[j], 2))  # 共同交互过i, j的用户对
        result = 0
        for (u, v) in user_pairs:
            result += 1 / (alpha + list(u_items[u] & u_items[v]).__len__())  # \frac1{\alpha+|I_u\cap I_v|},
        if result != 0:
            item_sim_dict.setdefault(i, dict())  # 初始化物品i
            item_sim_dict[i][j] = format(result, '.6f')
    return item_sim_dict  # 返回item相似度字典, key为item_id, value为与该item相似的item_id及相似度


def save_item_sims(item_sim_dict, top_k, path):
    new_item_sim_dict = dict()
    try:
        writer = open(path, 'w', encoding='utf-8')
        for item, sim_items in item_sim_dict.items():
            new_item_sim_dict.setdefault(item, dict())
            new_item_sim_dict[item] = dict(
                sorted(sim_items.items(), key=lambda k: k[1], reverse=True)[:top_k])  # 排序取出 top_k个相似的item
            writer.write('item_id:%d\t%s\n' % (item, new_item_sim_dict[item]))
        print("SUCCESS: top_{} item saved".format(top_k))
    except Exception as e:
        print(e.args)


if __name__ == "__main__":
    train_data_path = "input/ratings_final.txt"
    item_sim_save_path = "output/item_sim_dict.txt"
    alpha = 0.5
    top_k = 10  # 与item相似的前 k 个item
    train = load_data(train_data_path)
    u_items, i_users = get_uitems_iusers(train)
    item_sim_dict = swing_model(u_items, i_users, alpha)
    save_item_sims(item_sim_dict, top_k, item_sim_save_path)
```
输出：
```
item_id:10	{33: '9.369308', 31: '7.928368', 47: '5.119530', 29: '4.917776', 26: '4.831854', 5: '4.683046', 44: '4.008186', 34: '3.672097', 39: '3.564048', 35: '3.440806'}
item_id:40	{39: '6.158427', 23: '4.632332', 33: '4.158849', 31: '3.806523', 29: '3.648068', 37: '3.619602', 44: '3.429525', 47: '3.339190', 13: '2.779905', 38: '2.730002'}
item_id:38	{24: '6.053660', 34: '5.870747', 37: '3.573692', 16: '3.442025', 22: '3.350414', 44: '3.142554', 33: '3.035718', 19: '2.920647', 1: '2.838041', 43: '2.728516'}
item_id:41	{24: '6.010128', 35: '5.212512', 13: '4.167028', 33: '4.107981', 42: '3.528947', 31: '3.508580', 46: '3.186477', 5: '3.029635', 12: '2.768056', 16: '2.493831'}
item_id:1	{31: '7.446685', 33: '5.682953', 26: '5.509304', 25: '4.267118', 19: '3.768161', 42: '3.599587', 34: '3.567465', 47: '3.367748', 13: '3.305826', 29: '3.260593'}
item_id:19	{47: '7.296887', 29: '5.516440', 31: '4.385619', 34: '4.169741', 5: '3.913757', 13: '3.611356', 44: '3.491264', 28: '3.002532', 33: '2.969671', 35: '2.744241'}
item_id:50	{33: '8.690459', 27: '5.078108', 29: '4.640582', 47: '4.618278', 46: '4.478693', 35: '3.572142', 26: '3.049875', 13: '3.017851', 16: '2.997565', 44: '2.921754'}
item_id:49	{44: '3.339786', 29: '2.908996', 22: '2.822287', 26: '2.804519', 8: '2.512262', 14: '2.290975', 35: '2.152327', 46: '1.932279', 47: '1.859695', 5: '1.851126'}
item_id:18	{33: '9.920042', 39: '4.963052', 13: '3.424331', 44: '3.095113', 26: '3.067319', 28: '3.026770', 42: '2.541916', 11: '2.273064', 27: '2.131459', 12: '1.968478'}
item_id:17	{35: '4.230042', 44: '3.490393', 33: '2.887144', 16: '2.579394', 34: '2.490362', 29: '1.976302', 39: '1.716705', 27: '1.653835', 46: '1.594548', 15: '1.590245'}
item_id:29	{15: '7.765038', 31: '7.699624', 13: '7.647818', 23: '7.624496', 47: '6.841698', 26: '6.210281', 8: '6.187537', 39: '5.725741', 24: '5.666022', 6: '5.411497'}
item_id:36	{24: '5.823616', 37: '4.367127', 47: '4.106552', 12: '2.964950', 42: '2.911300', 33: '2.708663', 25: '2.404134', 16: '2.350667', 31: '2.273338', 13: '2.137498'}
item_id:4	{31: '5.609852', 44: '4.414842', 33: '4.036755', 24: '4.017716', 39: '2.561788', 47: '2.521624', 13: '2.157043', 26: '1.741391', 35: '1.683751', 15: '1.653737'}
item_id:45	{26: '5.482775', 35: '4.646579', 34: '4.081938', 11: '3.202662', 13: '3.001079', 30: '2.863475', 39: '2.676639', 24: '2.616236', 2: '2.573928', 5: '2.481827'}
item_id:27	{31: '5.421499', 26: '3.988214', 33: '3.633894', 21: '3.493322', 5: '3.339473', 16: '3.085929', 24: '2.921068', 35: '2.631777', 47: '2.299567', 28: '2.220544'}
item_id:44	{33: '7.679608', 35: '4.741540', 31: '4.397215', 37: '4.138681', 34: '4.053771', 39: '4.050300', 46: '3.907530', 48: '3.647042', 26: '3.598477', 15: '3.300210'}
item_id:26	{34: '6.816138', 31: '6.682804', 39: '5.334259', 33: '4.829264', 25: '4.774081', 5: '4.623790', 35: '4.499845', 3: '4.092521', 8: '3.597685', 13: '3.552098'}
item_id:37	{24: '6.843369', 47: '6.510498', 12: '6.430014', 43: '4.463759', 16: '4.037183', 31: '3.804751', 11: '3.745736', 33: '3.483176', 13: '3.426464', 25: '3.370829'}
item_id:21	{24: '5.424326', 33: '4.635283', 16: '3.070736', 31: '3.033045', 5: '3.022268', 34: '2.890594', 12: '2.462445', 2: '2.203518', 25: '1.944303', 47: '1.884469'}
item_id:39	{34: '5.951387', 11: '4.709477', 35: '4.594422', 24: '3.859427', 31: '3.639452', 28: '3.612227', 5: '3.518218', 8: '3.281585', 13: '3.192304', 25: '2.769550'}
item_id:48	{34: '4.169734', 47: '3.352142', 33: '2.325230', 31: '2.073971', 24: '1.853498', 43: '1.662014', 15: '1.615838', 5: '1.573125', 8: '1.377898', 23: '0.962713'}
item_id:33	{47: '9.778090', 31: '8.583368', 24: '7.860965', 42: '7.855050', 13: '7.480130', 35: '7.015999', 5: '6.818251', 12: '6.764609', 11: '6.272421', 43: '6.185646'}
item_id:31	{16: '7.105310', 5: '6.804654', 34: '6.040925', 42: '5.351108', 12: '5.088411', 13: '4.974254', 6: '4.522709', 47: '4.510200', 25: '4.144825', 24: '4.031206'}
item_id:8	{34: '7.980686', 22: '6.623847', 13: '3.555553', 24: '3.101205', 23: '3.059310', 6: '2.255133', 16: '2.133014', 46: '2.065485', 11: '2.017622', 42: '1.994395'}
item_id:42	{13: '7.610054', 47: '6.245807', 46: '4.083513', 24: '3.970524', 28: '2.697660', 16: '2.691497', 32: '2.042380', 22: '1.950787', 2: '1.864192', 11: '1.760448'}
item_id:22	{34: '6.418238', 47: '3.978257', 16: '3.115300', 46: '3.068168', 32: '2.713828', 13: '2.024774', 24: '1.994894', 6: '1.452366', 23: '1.434605', 12: '1.264802'}
item_id:46	{35: '6.741871', 24: '6.465305', 13: '5.543031', 47: '3.959384', 16: '3.904737', 14: '2.100597', 34: '2.009843', 23: '1.665809', 43: '1.365662', 9: '1.318219'}
item_id:24	{13: '9.051286', 12: '8.932852', 16: '7.994105', 47: '7.056703', 25: '5.138607', 34: '4.107616', 5: '3.916675', 23: '3.865081', 35: '3.327251', 11: '2.944826'}
item_id:30	{16: '4.278808', 34: '2.566232', 47: '1.778311', 3: '1.582892', 23: '1.169706', 11: '1.074682', 15: '1.071165', 6: '0.860447', 12: '0.638951', 5: '0.614136'}
item_id:32	{47: '2.562393', 11: '1.757731', 28: '1.635400', 5: '1.569821', 13: '1.463118', 23: '1.066609', 43: '1.011198', 12: '0.930438', 25: '0.924289', 35: '0.776113'}
item_id:14	{35: '2.999675', 16: '2.486331', 6: '2.298938', 12: '2.042664', 34: '1.674629', 25: '1.556820', 13: '1.454499', 28: '1.286698', 2: '1.209419', 7: '1.176068'}
item_id:43	{5: '6.015474', 16: '5.777772', 47: '4.198979', 35: '3.899262', 34: '2.491394', 15: '2.108719', 2: '1.653933', 28: '1.527717', 13: '1.513159', 23: '1.143745'}
item_id:28	{47: '4.465181', 34: '2.392612', 16: '2.009888', 35: '2.006491', 2: '1.871937', 13: '1.650812', 23: '1.515468', 5: '1.227799', 25: '1.181200', 3: '1.157692'}
item_id:7	{13: '1.737218', 35: '1.722211', 6: '1.688831', 16: '1.373115', 11: '1.012005', 25: '0.942200', 9: '0.825241', 12: '0.781583', 20: '0.404827', 34: '0.384314'}
item_id:2	{25: '2.863793', 13: '2.409164', 35: '2.256263', 34: '2.223412', 5: '1.959987', 3: '1.756192', 23: '1.414545', 16: '1.289138', 15: '1.263231', 12: '0.748327'}
item_id:11	{12: '6.191016', 13: '3.205327', 35: '2.463550', 47: '2.132076', 16: '1.844691', 5: '1.691581', 34: '1.441119', 6: '0.977622', 9: '0.929768', 23: '0.866365'}
item_id:47	{34: '7.172651', 13: '6.306686', 16: '5.733066', 23: '5.680964', 5: '4.475031', 20: '3.314331', 12: '2.738435', 3: '1.958704', 15: '1.652260', 9: '1.638983'}
item_id:5	{12: '6.600787', 13: '5.907053', 34: '4.839141', 25: '4.743027', 35: '3.908568', 15: '2.732167', 6: '1.742200', 23: '1.735456', 16: '1.714101', 3: '1.681806'}
item_id:35	{12: '4.986266', 13: '4.623114', 16: '3.369654', 34: '2.442132', 9: '2.008932', 25: '1.866088', 15: '1.683837', 6: '0.799530', 3: '0.778766', 20: '0.153846'}
item_id:34	{16: '6.049411', 15: '5.894930', 6: '5.486742', 23: '5.281373', 13: '3.893850', 25: '3.873124', 12: '2.577946', 3: '2.081190', 20: '1.876311', 9: '0.382418'}
item_id:23	{13: '3.898035', 6: '2.681844', 3: '1.643481', 16: '1.295438', 15: '1.281638', 12: '0.598291', 25: '0.117647', 20: '0.117647', 9: '0.095238'}
item_id:25	{13: '5.461985', 12: '4.039666', 20: '1.690132', 6: '1.649728', 16: '1.481843', 9: '0.694180', 15: '0.559179', 3: '0.556820'}
item_id:12	{13: '6.455876', 6: '3.627968', 20: '2.449737', 16: '2.022631', 9: '1.332711', 3: '0.295739'}
item_id:15	{13: '2.303888', 3: '1.099235', 16: '1.020237', 6: '0.783844', 9: '0.692284', 20: '0.286959'}
item_id:20	{6: '2.109553', 16: '1.629160', 13: '0.849959', 3: '0.153846', 9: '0.133333'}
item_id:16	{6: '2.801347', 13: '1.961959', 3: '0.964732', 9: '0.420513'}
item_id:3	{13: '0.619418', 6: '0.133333'}
item_id:9	{13: '1.410797', 6: '0.133333'}
item_id:6	{13: '0.389140'}
```
</details>

### $\color{#39c5bb}{2.1.6\ surprise算法}$

<p style="color:#EC407A; font-weight:bold">1. 类别层面</p>

$$\theta_{i,j}=p(c_{i,j}|c_j)=\frac{N(c_{i,j})}{N(c_j)}$$
- 其中，$N(c_{i,j})$为在购买过$i$之后购买j类的数量，$N(c_j)$为购买$j$类的数量。类似kmeans的inertia要看拐点，surprise的inertia要看曲线的拐点来选择相关类的个数。

<p style="color:#EC407A; font-weight:bold">2. 商品层面</p>

$$s_1(i,j)=\frac{\sum_{u\in U_i\cap U_j}1/(1+|t_{ui}-t_{uj}|)}{\|U_i\|\times\|U_j\|}$$
- 其中，$j$属于$i$的相关类 , 且$j$的购买时间晚于$i$。要注意购买顺序(前对后有影响)，购买时间(间隔越短越说明互补)

<p style="color:#EC407A; font-weight:bold">3. 聚类层面</p>

$$s_2(i,j)=s_1(L(i),L(j))$$

- 其中$L(i)$表示商品$i$所属的类别。
- 如何聚类？ 传统的聚类算法（k-means、DBSCAN）在数十亿产品规模下的淘宝场景中不可行，所以[作者](https://arxiv.org/pdf/2010.05525)采用了标签传播算法。
- 在哪里标签传播？ Item-item图，其中由Swing计算的排名靠前item为邻居，边的权重就是Swing分数。
- 表现如何？ 快速而有效，15分钟即可对数十亿个项目进行聚类。

<p style="color:#EC407A; font-weight:bold">4. 线性组合</p>

$$s(i, j)=\omega * s_{1}(i, j)+(1-\omega) * s_{2}(i, j)$$

- 其中，$\omega=0.8$是作者设置的权重超参数。

Surprise算法通过利用类别信息和标签传播技术解决了用户共同购买图上的稀疏性问题。


### $\color{#39c5bb}{2.1.7\ 矩阵分解 MF(matrix \ factorization)}$

<p style="color:#EC407A; font-weight:bold">1. 预测打分 </p>

假设$n$个用户，$M$部电影有$D$个特征。我们预测
$$R_{n\times M}=P_{n\times D}\times Q_{D\times M}$$
- 式中$R$(Rating)是预测$n$个用户对$M$部电影的**评分**，$P$(Preference)是**用户喜好**矩阵，$Q$(Quality)是**电影特征**矩阵。

例如：
P矩阵：
| |特征$D_1$|特征$D_2$|特征$D_3$|
|---|---|---|---|
|用户$p_1$|0.1|0.9|0.6|
|用户$p_2$|0.8|0.5|0.4|

Q矩阵：
| |电影$Q_1$|电影$Q_2$|电影$Q_3$|电影$Q_4$|
|---|---|---|---|---|
|特征$D_1$|0.7|0.2|0.3|0.4|
|特征$D_2$|0.1|0.6|0.9|0.2|
|特征$D_3$|0.5|0.8|0.4|0.1|

因此：
$$R=\begin{bmatrix}
0.1&0.9&0.6\\
0.8&0.5&0.4
\end{bmatrix}\times
\begin{bmatrix}
0.7&0.2&0.3&0.4\\
0.1&0.6&0.9&0.2\\
0.5&0.8&0.4&0.1
\end{bmatrix}
= \begin{bmatrix}
0.46&1.04&1.08&0.28\\
0.81&0.78&0.85&0.46
\end{bmatrix}$$

即：

|        |电影$Q_1$|电影$Q_2$|电影$Q_3$|电影$Q_4$|
|--------|-----|-----|-----|-----|
|用户$p_1$|0.46 |1.04| **1.08**| 0.28|
|用户$p_2$|0.81 |0.78| **0.85**| 0.46|


<p style="color:#EC407A; font-weight:bold">2. 反推用户、物品矩阵 </p>

**2.1 FunkSVD**(Latent Factor Model, LFM)
因为实际获取到的一般是稀疏矩阵$R_{n\times M}$，我们需要反推$P_{n\times D}$,$Q_{D\times M}$。因此损失函数就是预测的$R^{\prime}$与真实的$R$之间的误差，可选MSE。注意正则化的时候需要考虑每个用户对电影打分的数量不同，因此损失函数是：
$$ J(P,Q)=\frac12\sum_{i=1}^n\sum_{j=1}^M\mathbb{I}_{ij}\left((r_{ij}^{\prime}-r_{ij})^2+\lambda(\|\boldsymbol{p}^{(i)}\|^2+\|\boldsymbol{q}_j\|^2)\right)$$
- 其中，用户$i$对电影$j$的评分$r_{ij}^{\prime}$是用户偏好与电影特征的内积$r_{ij}^{\prime}=\boldsymbol{p}^{(i)}\boldsymbol{q}_j$，$\lambda$受示性函数$\mathbb{I}$控制，$\mathbb{I}_{ij}$代表用户$i$是否对电影$j$评分，$\boldsymbol{p}^{(i)}$是用户$i$的特征向量，$\boldsymbol{q}_j$是电影$j$的特征向量。
向量内积是**双线性函数**bilinear model。双线性的含义为，二元函数固定任意一个自变量时，函数关于另一个自变量线性。

**2.2 BiasSVD**（Bias Latent Factor Model）
在FunkSVD的基础上，加入用户偏好和物品偏好，即：
$$r_{ij}^{\prime}=\mu+b_u+b_i+\boldsymbol{p}^{(i)}\boldsymbol{q}_j$$
- 其中，$\mu$是全局平均评分，一般使用所有样本评分的均值；$b_u$是用户偏好，是用户$u$给出的所有评分的均值；$b_i$是物品偏好，是物品$i$得到的所有评分的均值。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开查看完整代码</span></summary>

```python
import random
import math


class BiasSVD:
    def __init__(self, rating_data, F=5, alpha=0.1, lmbda=0.1, max_iter=1000):
        self.F = F  # 这个表示隐向量的维度
        self.P = dict()  # 用户矩阵P  大小是[users_num, F]
        self.Q = dict()  # 物品矩阵Q  大小是[item_nums, F]
        self.bu = dict()  # 用户偏置系数
        self.bi = dict()  # 物品偏置系数
        self.mu = 0  # 全局偏置系数
        self.alpha = alpha  # 学习率
        self.lmbda = lmbda  # 正则项系数
        self.max_iter = max_iter  # 最大迭代次数
        self.rating_data = rating_data  # 评分矩阵

        for user, items in self.rating_data.items():
            # 初始化矩阵P和Q。随机数根据经验，和1/sqrt(F)成正比
            self.P[user] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
            self.bu[user] = 0
            for item, rating in items.items():
                if item not in self.Q:
                    self.Q[item] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
                    self.bi[item] = 0

    # 采用随机梯度下降的方式训练模型参数
    def train(self):
        cnt, mu_sum = 0, 0
        for user, items in self.rating_data.items():
            for item, rui in items.items():
                mu_sum, cnt = mu_sum + rui, cnt + 1
        self.mu = mu_sum / cnt

        for step in range(self.max_iter):
            # 遍历所有的用户及历史交互物品
            for user, items in self.rating_data.items():
                # 遍历历史交互物品
                for item, rui in items.items():
                    rhat_ui = self.predict(user, item)  # 评分预测
                    e_ui = rui - rhat_ui  # 评分预测偏差
                    # 参数更新
                    # $$b_u=b_u+\alpha(e_{ui}-\lambda b_u)$$
                    # $$b_i=b_i+\alpha(e_{ui}-\lambda b_i)$$
                    self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])
                    self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])
                    for k in range(0, self.F):
                        # p_{u,k}=p_{u,k}+\alpha e_{ui}q_{i,k}-\lambda p_{u,k}
                        # q_{i,k}=q_{i,k}+\alpha e_{ui}p_{u,k}-\lambda q_{i,k}
                        self.P[user][k] += self.alpha * (e_ui * self.Q[item][k] - self.lmbda * self.P[user][k])
                        self.Q[item][k] += self.alpha * (e_ui * self.P[user][k] - self.lmbda * self.Q[item][k])
            # 逐步降低学习率
            self.alpha *= 0.9

    # 评分预测
    def predict(self, user, item):
        # $$r_{ij}^{\prime}=\mu+b_u+b_i+\boldsymbol{p}^{(i)}\boldsymbol{q}_j$$
        return sum(self.P[user][f] * self.Q[item][f] for f in range(0, self.F))\
            + self.bu[user] + self.bi[item] + self.mu


# 加载数据
rating_data = {'y': {'A': 5, 'B': 3, 'C': 4, 'D': 4},
               'X1': {'A': 3, 'B': 1, 'C': 2, 'D': 3, 'k': 3},
               'X2': {'A': 4, 'B': 3, 'C': 4, 'D': 3, 'k': 5},
               'X3': {'A': 3, 'B': 3, 'C': 1, 'D': 5, 'k': 4},
               'X4': {'A': 1, 'B': 5, 'C': 5, 'D': 2, 'k': 1}
               }
biassvd = BiasSVD(rating_data, F=10)
biassvd.train()
# 预测用户y对物品k的评分
for item in ['k']:
    print(item, biassvd.predict('y', item))
```

输出(不固定)：
```
k 4.998386122960065
```
</details>

**2.3 MF小结**
用户和物品都用隐向量的形式存放，空间复杂度从$O(n^2)$降到$O((n+M)\cdot D)$，其中$D$是隐向量的维度。MF模型的优点是可以发现用户和物品的隐含特征，但是也有一些缺点：
- 没有考虑到用户特征，物品特征和上下文特征。
- 在缺乏用户历史行为的时候，无法进行有效的推荐。

### $\color{39c5bb} 2.1.8\ 协同过滤小结 $

<p style="color:#EC407A; font-weight:bold">1. UserCF和ItemCF的区别</p>

- **UserCF**：适用于用户少，物品多，时效性较强的场合，比较的是人与人之间的相似度，具备更强的社交属性。可以发现用户潜在的尚未发现的兴趣。例如新闻等。

- **ItemCF**：适用于物品少，用户多，物品相对稳定，用户兴趣稳定的场合，比较的是物品与物品之间的相似度，具备更强的个性化推荐属性。例如电影、音乐等。
  - item相比于用户变化的慢，且新item特征比较容易获得，所以ItemCF在实际应用中更加广泛。

<p style="color:#EC407A; font-weight:bold">2. Swing与MF相较于传统CF的优势</p>

- **Swing**：通过用户共同购买的事件计算物品之间的相似度。

- **MF**：通过矩阵分解的方式，将用户-物品交互矩阵分解为两个低维矩阵的乘积，得到用户和物品的Embedding。


<p style="color:#EC407A; font-weight:bold">3. 协同过滤算法的权重改进</p>

- **base 公式**
$$w_{ij}=\frac{|N(i)\bigcap N(j)|}{|N(i)|}$$
  - 该公式表示同时喜好物品$i$和物品$j$的用户数，占喜爱物品$i$的比例。
  - 缺点：若物品$j$为热门物品，那么它与任何物品的相似度都很高。

- **对热门物品进行惩罚**
$$w_{ij}=\frac{|N(i)\cap N(j)|}{\sqrt{|N(i)||N(j)|}}$$
  - 根据 base 公式在的问题，对物品$j$进行打压。打压的出发点很简单，就是在分母再除以一个物品$j$被购买的数量
  - 此时，若物品$j$为热门物品，那么对应的$N(j)$也会很大，受到的惩罚更多。

- **控制对热门物品的惩罚力度**
$$w_{ij}=\frac{|N(i)\cap N(j)|}{|N(i)|^{1-\alpha}|N(j)|^\alpha}$$
  - 除了第二点提到的办法，在计算物品之间相似度时可以对热门物品进行惩罚外。
  - 可以在此基础上，进一步引入参数 $\alpha$ ,这样可以通过控制参数 $\alpha$来决定对热门物品的惩罚力度。

- **对活跃用户的惩罚**
$$w_{ij}=\frac{\sum_{\mathrm{u\in N(i)\cap N(j)}}\frac1{\log1+|N(u)|}}{|N(i)|^{1-\alpha}|N(j)|^\alpha}$$
  - 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。
  - 对于异常活跃的用户，在计算物品之间的相似度时，他的贡献应该小于非活跃用户。

<p style="color:#EC407A; font-weight:bold">3. 协同过滤算法的问题</p>

- **泛化能力弱**
  - 无法将两个物品相似的信息推广到其他物品的相似性上。
- **头部效应明显， 处理稀疏向量的能力弱**
  - 热门物品具有很强的头部效应，容易跟大量物品产生相似，而尾部物品由于特征向量稀疏，导致很少被推荐。


## $\color{#39c5bb}{2.2\ 基于向量的召回}$
Embedding就是用一个低维、稠密的向量表示一个对象，相当于对One-hot编码做了平滑处理。它将用户和物品映射到低维空间，通过内积计算相似度。
主要分为**i2i**和**u2u**两种召回，i2i得到的是物品的embedding，u2u得到的是用户和物品的embedding。


<div style="text-align: center;">
    <img src="assets/推荐系统/images/image-6.png" width="400px">
    <img src="assets/推荐系统/images/image-7.png" width="400px">
</div>

### $\color{#39c5bb}{2.2.1\ Word2Vec}$
分为Skip-gram和CBOW两种模型。
- **Skip-gram**：通过中心词预测上下文
- **CBOW**：通过上下文预测中心词

<div style="text-align: center;">
    <img src="assets/推荐系统/images/image-8.png" width="600px">
</div>

计算过程对比：
左图是根据中心词`believe`预测上下文的过程，右图是根据上下文预测中心词`believe`的过程。
<div style="text-align: center;">
    <img src="assets/推荐系统/images/image-9.png" width="400px">
    <img src="assets/推荐系统/images/image-10.png" width="400px">
</div>

设独热编码矩阵为$V$，中心词向量为$V_i$，上下文词向量为$U_j$。
$T$是文本长度，$c$是上下文窗口大小，$w_t$是中心词，$w_{t+j}$是上下文词，$c \in C$具体而言是$-c\leq j\leq c,j\ne 0$。
公式如下：
<p style="color:#EC407A; font-weight:bold">1. Skip-gram</p>

Skip-gram的损失函数为：
$$J=-\frac1T\sum_{t=1}^T\sum_{c \in C}\log p(w_{t+j}|w_t)$$
其中，$p(w_{t+j}|w_t)=\frac{exp(u_j^Tv_t)}{\sum_{i=1}^Vexp(u_i^Tv_t)}$，$u_j$是上下文词的向量, $v_t$是中心词的向量。

<p style="color:#EC407A; font-weight:bold">2. CBOW</p>

CBOW的损失函数为：
$$J=-\frac1T\sum_{t=1}^T\log p(w_t|w_{t-c},\cdots,w_{t+c})$$
其中，$p(w_t|w_{t-c},\cdots,w_{t+c})=\frac{exp(v_t^T\sum_{c \in C}u_{w_{t+j}})}{\sum_{i=1}^Vexp(v_i^T\sum_{c \in C}u_{w_{t+j}})}$，$u_{w_{t+j}}$是上下文词的向量, $v_t$是中心词的向量。


