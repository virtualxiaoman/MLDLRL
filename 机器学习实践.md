参考书籍：
- [《机器学习高级实践·计算广告、供需预测、智能营销、动态定价》机械工业出版社](https://book.douban.com/subject/36648547/)
- [《机器学习算法竞赛实战》](https://book.douban.com/subject/35595757/)，
  - 作者[知乎主页](https://www.zhihu.com/people/wang-he-13-93)

---

# 前言

算是又开了个坑吧，之前写过B站的Recommendation System，但是理论很美好，实践出大问题。最严重的问题是正负类比例严重失衡，而且SMOTE没有丝毫作用，所以打算先学习他人的处理方法。

本文分为小型demo与项目实践两部分。
小型demo可能是对于某些数据集的解决方案，而项目实践则是对于某个项目的解决方案。比如kaggle上的好例子可能归为小型demo，而比较大的项目或者是某一类数据的分析方法则归为项目实践。

如果是他人的项目，会使用@引用。如果是自己的项目，会使用©。

---

# 1. 小型demo

## 1.1 House Price - Advanced Regression Techniques 
@Kaggle, 《机器学习算法竞赛实战》

[数据集](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)

### 1.1.1 数据集介绍

`train shape: (1460, 81), test shape: (1459, 80)`，除`ID`与`SalePrice`外，还有79个特征。

提交是根据RMSE进行评估的，所以score越小越好。

已有的研究：[kaggle](https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python), [kaggle](https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard), [kaggle](https://www.kaggle.com/code/jimmyyeung/house-regression-beginner-catboost-top-2)

请注意参考书籍的P28~P29代码有错，他没有把`train['SalePrice']`从`X_train`中删除，导致score很高(0.4左右)。

本文的综述：

| 序号 | 代码 | 描述 | 模型 | score |
| --- | --- | --- | --- | --- |
| 1 | [D_HP_1_quick_start.py](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_1_quick_start.py) | 快速开始 | `lgb`+`KFold` | 0.13449 |
| 2 | [D_HP_2_EDA.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_2_EDA.ipynb) | 数据分析与可视化 | `train['SalePrice'].mean()` | 0.42577 |
| 3 | [D_HP_3_data_process.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_3_data_process.ipynb) | 数据预处理 | 1的基础加上数据预处理 | 0.13442 |
| 4 | [D_HP_4_DNN.py](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_4_DNN.py) | Linear, CNN, Transformer | `CNN` | 0.19564 |
| 5 | [D_HP_5_stack_model.py](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_5_stack_model_by_GPT.py) | 堆叠模型 | `StackingRegressor` | 0.13145 |
| 6 | [D_HP_6_process_and_stack.py](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Demo_House_Prices/D_HP_6_process_and_stack.py) | 预处理+堆叠 | 预处理+堆叠+异常值 | 0.12510 |

`quick_start`相当于一个baseline，什么都不干直接lgb。`EDA`使用均值是作为一个下限的对照。`data_process`加上了数据处理。`DNN`是看看不加以设计的神经网络与lgb的差异。`stack_model`堆叠模型。`process_and_stack`是预处理+堆叠模型。


### 1.1.2 数据预处理

#### 1.1.2.1 缺失值处理

```python
stats = []
for col in train.columns:
    stats.append((col, train[col].nunique(), train[col].isnull().sum() * 100 / train.shape[0],
                  train[col].value_counts(normalize=True, dropna=False).values[0] * 100, train[col].dtype))
stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values',
                                        'Percentage of values in the biggest category', 'type'])
stats_df.sort_values('Percentage of missing values', ascending=False)
```
这段代码可以查看每个特征的缺失值情况、唯一值数量、最大类别占比、数据类型，挺好用的。

#### 1.1.2.2 了解特征

后面的EDA都用这个代码，简单好用就是运行时间好长。

<div style="display: flex; justify-content: center; align-items: center;">
<div style=" max-height: 200px; max-width: 90%; overflow-y: auto; border: 1px solid #39c5bb; border-radius: 10px;">

```python
import matplotlib.pyplot as plt
import seaborn as sns
import math


def describe_and_visualize(df, table_name):
    print("\n[log] -------------------- \n")
    print(f"分析表格：{table_name}")
    print(f"表格的形状：{df.shape}")

    categorical_columns = []  # 类别型字段
    numerical_columns = []  # 数值型字段

    # 分类字段与数值字段区分
    for column in df.columns:
        if df[column].dtype == 'object' or df[column].nunique() < 30:  # 类别型或唯一值较少
            categorical_columns.append(column)
        else:
            numerical_columns.append(column)

    # 输出类别型字段的信息
    print(f"\n>>> 类别型字段（{len(categorical_columns)} 个）：{categorical_columns}")
    for column in categorical_columns:
        print(f"字段：{column}")
        print(f"{column}是类别型数据，共有{df[column].nunique()}个不同的值")
        print(df[column].value_counts())

    # 绘制类别型字段的条形图
    if categorical_columns:
        print("\n[log] 正在绘制类别型字段的统计图...")
        rows = math.ceil(len(categorical_columns) / 4)
        fig, axes = plt.subplots(rows, min(len(categorical_columns), 4), figsize=(20, 5 * rows))
        axes = axes.flatten()  # 展平方便处理
        for i, column in enumerate(categorical_columns):
            df[column].value_counts().plot(kind='bar', ax=axes[i], title=f'{column} - Count Values')
            axes[i].set_xlabel(column)
            axes[i].set_ylabel('Frequency')
        # 隐藏多余的子图
        for j in range(len(categorical_columns), len(axes)):
            axes[j].axis('off')
        plt.tight_layout()
        plt.show()

    # 输出数值型字段的信息
    print(f"\n>>> 数值型字段（{len(numerical_columns)} 个）：{numerical_columns}")
    for column in numerical_columns:
        print(f"\n字段：{column}")
        print(f"{column}是数值型数据，共有{df[column].nunique()}个不同的值")
        print(df[column].describe())

    # 绘制数值型字段的分布图
    if numerical_columns:
        print("\n[log] 正在绘制数值型字段的分布图...")
        rows = math.ceil(len(numerical_columns) / 4)
        fig, axes = plt.subplots(rows, min(len(numerical_columns), 4), figsize=(20, 5 * rows))
        axes = axes.flatten()  # 展平方便处理
        for i, column in enumerate(numerical_columns):
            sns.histplot(df[column], kde=True, bins=30, ax=axes[i])
            axes[i].set_title(f'{column} - Distribution')
            axes[i].set_xlabel(column)
            axes[i].set_ylabel('Frequency')
        # 隐藏多余的子图
        for j in range(len(numerical_columns), len(axes)):
            axes[j].axis('off')
        plt.tight_layout()
        plt.show()
```

</div>
</div>

得到的可视化如下：


<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-1.png" style="width: 70%;"/>
        <p style="font-size: small; color: gray;">离散型</p>
    </div>
</div>

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-2.png" style="width: 70%;"/>
        <p style="font-size: small; color: gray;">连续型</p>
    </div>
</div>

其中，我们的目标SalePrice有663个不同的值，分布如下：

| count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1460 | 180921.2 | 79442.5 | 34900 | 129975 | 163000 | 214000 | 755000 |

观察上面的曲线可以看出近似正态分布，但有些右偏（毕竟没有低于0的房价，所以右偏很正常）。通过计算`train['SalePrice'].skew()`也可以知道Skewness为1.883。下面是各种方法的Skewness值：

| 方法 | Skewness |
| --- | --- |
| `np.log1p(train['SalePrice'])` | 0.1213 |
| `np.sqrt(train['SalePrice'])` | 0.9432 |
| `stats.boxcox(train['SalePrice'])[0]` | -0.0087 |
| `1 / train['SalePrice']` | 2.1421 |

> 偏态系数公式为：
$$
\text{Skewness} = \frac{n}{(n-1)(n-2)} \sum_{i=1}^{n} \left( \frac{x_i - \bar{x}}{s} \right)^3
$$
若Skewness<0则称为**左偏** /负偏态/左倾斜，即数据分布的**左尾较长**，大部分的数据值集中在分布的右侧(峰在右侧)，满足$\mu < \text{median} < \text{mode}$。如：考试中大部分学生得分很高，但少数学生得分很低；
若Skewness>0则称为**右偏** /正偏态/右倾斜，即数据分布的**右尾较长**，峰在左侧，满足$\mu > \text{median} > \text{mode}$。如：个人收入分布，大部分人收入较低，但少数人收入很高。

#### 1.1.2.3 corr

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-3.png" style="width: 70%;"/>
    </div>
</div>

#### 1.1.2.4 多变量分析

分析特征与特征之间的关系，常常是类别型数据绘制`bar`图，数值型数据绘制`scatter`图等。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-4.png" style="width: 80%;"/>
        <p style="font-size: small; color: gray;">'Neighborhood', 'OverallQual'</p>
    </div>
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-5.png" style="width: 80%;"/>
        <p style="font-size: small; color: gray;">'Neighborhood', 'SalePrice'</p>
    </div>
</div>

注意到NoRidge, NridgHt, StoneBr的评价不错，房屋价格也较高(最高的三个)。也就是：高评价的地区房价较高，所以属性`Neighborhood`和`OverallQual`可以组合起来构建交叉特征使用。

#### 1.1.2.5 Ridge回归与Lasso回归

`Ridge`取`alphas = [0.005, 0.01, 0.05, 0.1, 0.3, 1, 3]`。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-6.png" style="width: 50%;"/>
        <p style="font-size: small; color: gray;">其中的最优值是0.05</p>
    </div>
</div>

`Lasso`取`alphas=[0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 1]`，其中的最优值是0.001，有39个特征的系数被压缩为0，53个特征的系数绝对值小于0.01。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image-7.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">选取最高和最低各10个。书上那个图感觉作假了。。</p>
    </div>
</div>

Ridge 回归(圆形区域约束，系数被压缩)适用于特征较多、共线性较高的情况，能够减小方差。
Lasso 回归(菱形区域约束，解容易落在坐标轴上)适用于特征较多、稀疏性较强(无关特征多)，需要自动特征选择的情况。

> **共线性**：自变量之间存在高度相关性。此时比如系数分别是2,-1和100,-99是一样的。而OLS的线性解是$\hat{\beta} = (X^TX)^{-1}X^TY$，特征之间的高度相关性会导致$(X^TX)$的行列式接近0，并且$(X^TX)^{-1}$的元素会变得很大，使得对应的$\hat{\beta}$也会变得很大。

#### 1.1.2.6 特征工程

删除缺失值比例大于50%的特征列，使用unknown和中位数填充缺失值。删除类别比不均衡的特征。
修改错误值：将销售日期小于建造日期的数据的销售日期改为2009(销售日期的最大值)。增加新列：对浴池求和得到地下室的总浴室数等。
object编码。

### 1.1.3 模型对比

#### 1.1.3.1 神经网络

原理不赘述，总之PyTorch启动。当然我个人不喜欢什么都不管就直接启动的，这里只是作为一个对比的示例。

#### 1.1.3.2 堆叠模型

很奇怪啊，书上说能跑到0.13157，但我运行他给的代码是0.4几的分数，而且模型的预测值几乎一模一样，我看了他的ipynb的输出跟我的也是基本一样的，感觉他又造假了。然后我让GPT写了个，直接0.13145。

#### 1.1.3.3 预处理+stack

1. 重构之前的代码但不改变任何参数，得到0.13086。相比于堆叠模型，这个分数几乎只好了一点点。
2. 修改模型参数，分数基本不下降(在0.14左右)，估计是原作者把参数调到差不多最优了。
3. 想起来了原书作者没做**异常值**，但按照“标准做法”设置为`z-score > 3`得到的是`0.14223`，注意到此时很多样本都被删除了，所以我改成`z-score > 8`，得到`0.12537`。


关于对数变化，可以参考[这里](https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers)。

https://chatgpt.com/c/678bb110-82a4-800f-8297-0dcfcb236187
https://kimi.moonshot.cn/chat/cu5r36vahd86t5i0p9qg

---



## 1.2 Elo Merchant Category Recommendation

[数据集](https://www.kaggle.com/competitions/elo-merchant-category-recommendation)


### 1.2.1 数据集介绍


本文的综述：

| 序号 | 代码 | 描述 | 模型 | private score | public score |
| --- | --- | --- | --- | --- | --- |
| 1 | [D_Elo_1_quick_start.py]() | 快速开始 | `lgb`+`KFold` | 3.80727 | 3.92136 |


1. train.csv 训练数据 (201917, 6) 8MB

| 属性 | 描述 | 第一行数据 |
| --- | --- | --- |
| card_id | 唯一的卡标识符 | C_ID_92a2005557 |
| first_active_month | 首次激活月份(格式：YYYY-MM) | 2017-06 |
| feature_1 | 匿名化的卡分类特征 | 5 |
| feature_2 | 匿名化的卡分类特征 | 2 |
| feature_3 | 匿名化的卡分类特征 | 1 |
| target | 忠诚度分数(在历史数据和评估期后两个月计算) | -0.82028260 |

test.csv 测试数据 (123623, 5)

2. historical_transactions.csv 历史交易记录 2.65GB

| 属性 | 描述 | 第一行数据 |
| --- | --- | --- |
| card_id | 卡标识符 | C_ID_4e6213e9bc |
| month_lag | 相对于参考日期的月份滞后 | -8 |
| purchase_date | 购买日期 | 2017-06-25 15:33:07 |
| authorized_flag | 授权标志('Y'批准，'N'拒绝) | Y |
| category_3 | 匿名化分类 | A |
| installments | 购买的分期付款次数 | 0 |
| category_1 | 匿名化分类 | N |
| merchant_category_id | 商户分类标识符(匿) | 80 |
| subsector_id | 商户分类组标识符(匿) | 37 |
| merchant_id | 商户标识符(匿) | M_ID_e020e9b302 |
| purchase_amount | 归一化的购买金额 | -0.70333091 |
| city_id | 城市标识符(匿) | 88 |
| state_id | 州标识符(匿) | 16 |
| category_2 | 匿名化分类 | 1.00000000 |

3. new_merchant_period.csv 新商户时期的交易记录 181MB

结构与 `historical_transactions.csv` 完全一致，数据来源为新商户时期的交易记录。

4. merchants.csv 商户信息 47.7MB

| 属性 | 描述 | 第一行数据 |
| --- | --- | --- |
| merchant_id | 唯一的商户标识符 | M_ID_838061e48c |
| merchant_group_id | 商户组标识符(匿) | 8353 |
| merchant_category_id | 商户分类标识符(匿) | 792 |
| subsector_id | 商户分类组标识符(匿) | 9 |
| numerical_1 | 匿名化数值指标 | -0.05747065 |
| numerical_2 | 匿名化数值指标 | -0.05747065 |
| category_1 | 匿名化分类 | N |
| most_recent_sales_range | 最近一个月收入范围A>B>C>D>E | E |
| most_recent_purchases_range | 最近一个月交易量范围A>B>C>D>E | E |
| avg_sales_lag3 | 最近3个月收入月均值 / 上月收入 | -0.40000000 |
| avg_purchases_lag3 | 最近3个月交易量月均值 / 上月交易量 | 9.66666667 |
| active_months_lag3 | 最近3个月内的活跃月数 | 3 |
| avg_sales_lag6 | 最近6个月收入月均值 / 上月收入 | -2.25000000 |
| avg_purchases_lag6 | 最近6个月交易量月均值 / 上月交易量 | 18.66666667 |
| active_months_lag6 | 最近6个月内的活跃月数 | 6 |
| avg_sales_lag12 | 最近12个月收入月均值 / 上月收入 | -2.32000000 |
| avg_purchases_lag12 | 最近12个月交易量月均值 / 上月交易量 | 13.91666667 |
| active_months_lag12 | 最近12个月内的活跃月数 | 12 |
| category_4 | 匿名化分类 | N |
| city_id | 城市标识符(匿) | 242 |
| state_id | 州标识符(匿) | 9 |
| category_2 | 匿名化分类 | 1.00000000 |


### 1.2.2 数据预处理

#### 1.2.2.1 正确性检验&分布差异检验

正确性主要检验card_id是否唯一：

```python
assert train['card_id'].nunique() == train['card_id'].shape[0]  # 训练集card_id唯一
assert test['card_id'].nunique() == test['card_id'].shape[0]  # 测试集card_id唯一
assert test['card_id'].nunique()+ train['card_id'].nunique()  == len(set(test['card_id'].values.tolist()+ train['card_id'].values.tolist()))  # card_id无重复
```

分布差异：函数`plot_feature_distributions()`。注意分布无差异也不能确保数据的生成方式相同，只是必要不充分条件。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/分布差异-first_active_month.png" style="width: 50%;"/>
        <p style="font-size: small; color: gray;">分布差异，以first_active_month为例，其余属性的差异比这个小</p>
    </div>
</div>


#### 1.2.2.2 可视化


对于**train.csv**：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/类别属性统计图.png" style="width: 80%;"/>
        <p style="font-size: small; color: gray;">train-类别属性统计图</p>
    </div>
</div>

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/数值属性统计图.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">train-数值属性统计图</p>
    </div>
</div>

注意到图中`target`有非常多`-33.21928095`，又注意到该属性在其他范围内都是连续型数据，因此这个大概率是异常值，或者说是固定值。

`target`共有197110个不同的值，统计量如下：

| count | mean | std | min | 25% | 50% | 75% | max |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 201917 | -0.393636 | 3.850500 | -33.219281 | -0.883110 | -0.023437 | 0.765453 | 17.965068 |

为了防止还有其余异常值，使用`train['target'].value_counts().head(10)`查看分布，发现除了-33.219281(2207个)外，还有0.000000(1630个)，这可能是缺失值填充的。


对于**merchants.csv**：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/类别属性统计图-merchant.png" style="width: 80%;"/>
        <p style="font-size: small; color: gray;">merchant-类别属性统计图</p>
    </div>
</div>

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/数值属性统计图-merchant.png" style="width: 80%;"/>
        <p style="font-size: small; color: gray;">merchant-数值属性统计图</p>
    </div>
</div>

`avg_purchases_lag3`, `avg_purchases_lag6`, `avg_purchases_lag12`三个都存在inf，另外的类别属性的分布也有很多极大的值，这可能是异常值。

缺失值有四列：
`category_2`缺失11887个(3.55%)，`avg_sales_lag3, avg_sales_lag6, avg_sales_lag12`都缺失13个(0.0039%)且缺失的行号相同`[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]`。

验证`merchant_id`是否唯一：

```python
print(f"merchant_id有重复的行行数：{merchants[merchants['merchant_id'].duplicated()==True].shape[0]}")
print(merchants[merchants['merchant_id'].duplicated()==True][['merchant_id', 'numerical_1']])  # 发现重复的行的记录似乎是一样的

duplicate_merchants = merchants[merchants.merchant_id.duplicated(keep=False)]  # 验证重复的行的记录是否一样
nunique_per_group = duplicate_merchants.groupby("merchant_id").nunique()  # 统计每个 merchant_id 在各列中的唯一值数量
inconsistent = nunique_per_group[nunique_per_group > 1].dropna(how="all").index.tolist()  # 筛选存在任意列唯一值数量大于1的商户ID
print("存在不一致记录的商户ID:", inconsistent)

duplicate_merchants.loc[duplicate_merchants["merchant_id"].isin(inconsistent)]  # 查看不一致记录，可以发现确实不一样
```

对于**historical_transactions.csv**和**new_merchant_period.csv**：

缺失值有三列：
`category_2`缺失2652864个(9.11%)，`category_3`缺失178159个(0.61%)，`merchant_id`缺失138481个(0.48%)。


#### 1.2.2.3 特征工程

<p style="color:#EC407A; font-weight:bold">1. 编码</p>

不同于`LabelEncoder`，原书按照数据的排序编码，引入了大小关系：

```python
def change_object_cols(se):
    value = se.unique().tolist()  # 将一维的Series转为list
    value.sort()  # 从小到大排序
    return se.map(pd.Series(range(len(value)), index=value)).values  # 生成一个Series，key是value，value是index，然后map到原Series上
```

<p style="color:#EC407A; font-weight:bold">2. 特征构建</p>

1. 对于时间数据的处理方法是：

```python
transaction['purchase_month'] = transaction['purchase_date'].apply(lambda x:'-'.join(x.split(' ')[0].split('-')[:2]))  # 取出年月，如2017-06-25 15:33:07 -> 2017-06
transaction['purchase_hour_section'] = transaction['purchase_date'].apply(lambda x: x.split(' ')[1].split(':')[0]).astype(int)//6  # 取出小时，然后分不同时段(0-5、6-11、12-17、18-23)
transaction['purchase_day'] = transaction['purchase_date'].apply(lambda x: datetime.strptime(x.split(" ")[0], "%Y-%m-%d").weekday())//5  # 取出星期几，然后分段(0-4、5-6)，区分工作日和周末的消费行为差异

transaction['purchase_day_diff'] = transaction.groupby("card_id")['purchase_day'].diff()  # 按照card_id分组，计算相邻数据的星期几的差值(当前行的值 - 上一行的值)
transaction['purchase_month_diff'] = transaction.groupby("card_id")['purchase_month'].diff()  # 计算相邻数据的月份的差值
```

2. 对于用户行为的统计：

```python
features = {}  # 用于存储特征
card_all = train['card_id'].append(test['card_id']).values.tolist()  # 所有的card_id
for card in card_all:
    features[card] = {}  # 为每个card_id创建一个字典，用于存储特征
    
columns = transaction.columns.tolist()  # 所有的列名
idx = columns.index('card_id')  # card_id的索引
category_cols_index = [columns.index(col) for col in category_cols]  # 类别型特征的索引
numeric_cols_index = [columns.index(col) for col in numeric_cols]  # 数值型特征的索引

for i in range(transaction.shape[0]):
    va = transaction.loc[i].values  # 取出第i行的数据
    card = va[idx]  # 取出card_id
    for cate_ind in category_cols_index:
        for num_ind in numeric_cols_index:
            col_name = '&'.join([columns[cate_ind], va[cate_ind], columns[num_ind]])  # 使用分类名分类值和数值值作为特征名
            features[card][col_name] = features[card].get(col_name, 0) + va[num_ind]  # 如果没有则get返回0，然后累加；否则get返回原值，然后累加
```

其`features`字典形如：

```python
{
    <card_id_1>: {
        <col_name_1>: <累加值_1>,
        <col_name_2>: <累加值_2>,
        ...
    },
}
```

3. 对于数值型数据使用`'nunique', 'mean', 'min', 'max','var','skew', 'sum'`进行统计，对于类别型数据使用`'nunique'`进行统计。

原书给的这部分代码没办法跑，需要的内存太多了，整了好多天，要优化的代码太多了，我的评价是感觉不如学开源代码。所以这里只做出了代码的解释，没有运行结果。



---


# 2. 项目实践

## 2.1 计算广告——广告点击率预估 
@《机器学习高级实践·计算广告、供需预测、智能营销、动态定价》

[数据集](https://tianchi.aliyun.com/dataset/56)

| 代码 | 描述 |
| --- | --- |
| [data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) | 数据预处理 |
| [data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) | 数据可视化 |

### 2.1.1 项目背景

#### 2.1.1.1 计算广告的目标

广告主(Demand,需求方)期望在线广告针对性更强，广告平台(Supply,供给方)期望广告点击率更高，用户期望广告更加个性化。
- **针对性**：广告能依据用户偏好精准地投放给潜在的、有需求的用户，以提高投入产出比。
- **点击率**：是广告投放效果的重要指标，是广告点击次数与广告曝光次数的比值。
- **个性化**：是指广告内容、形式、投放时间等因素能够根据用户的个性化需求进行定制。

#### 2.1.1.2 计算广告的术语

Computational Advertising：利用计算机技术、数学模型、统计方法等手段，通过对广告投放对象、广告内容、广告投放时机等进行精准分析，实现广告投放效果的最大化。

| 术语 | 全称 | 含义 |
| --- | --- | --- |
| **CTR** | Click-Through Rate, 点击率 | 点击次数÷曝光次数 |
| **CVR** | Conversion Rate, 转化率 | 转化次数÷点击次数，用户点击广告后完成特定行为Action(购买、下载等)的比例 |
| CPM | Cost Per Mille, 千次曝光成本 | 每千次曝光需支付的费用 |
| CPC | Cost Per Click, 单次点击成本 | 每次点击需支付的费用 |
| CPA | Cost Per Action, 单次行为成本 | 每次有效行为转化需支付的费用 |
| CPT | Cost Per Time, 单次时间成本 | 按播放时长计费 |
| **ROI** | Return On Investment, 投资回报率 | 收益÷成本 |

#### 2.1.1.3 计算广告的流程

<p style="color:#EC407A; font-weight:bold">1. 合约广告</p>

```mermaid
graph LR
    A[广告主与媒体签订合约] --> B[确定广告展示位置和时间]
    B --> C[广告上线与展示]
    C --> D[监控广告效果]
    D --> E[合约结束]
```

合约广告是单次交易，但粗粒度的广告投放方式会导致成本、收益不可控，不够理性。核心问题是：
1. 构建受众标签：聚类、分类、关联规则挖掘等
2. 事前流量预测：时序模型，如ARIMA、Prophet、LSTM、Transformer、DeepAR等
3. 在线流量分配：$\max \sum_{i=1}^{n} (r_i - c_i), \text{s.t.}  \sum_{i=1}^{n} d_i \leq D$，其中$r_i$是收入，$c_i$是成本，$d_i$是投放量，$D$是需求方的总投放量。转为为优化问题。

<p style="color:#EC407A; font-weight:bold">2. 竞价广告</p>

ADX(Ad Exchange)：广告交易平台，负责广告位的竞价、广告投放、广告效果监控等。
DSP(Demand Side Platform)：需求方平台。

```mermaid
graph LR
    A[用户访问网站] --> B[ADX发起广告位竞价并传输数据给DSP]
    B --> C[DSP出价]
    C --> D[ADX对DSP出价进行排序]
    D --> E[最高价的DSP获得广告位]
    E --> F[广告展示]
```

竞价广告是实时交易，是精细化的广告投放方式，但是需要解决的问题更多，尤其是CTR预估与实时性。

### 2.1.2 核心算法

```mermaid
graph LR
    A[广告库存] --> B[规则初筛]
    B --> C[粗排]
    C --> D[精排]
    D --> E[重排]
```

该部分内容在推荐系统中有详细介绍，这里不再赘述。

### 2.1.3 数据集介绍

[Ali_Display_Ad_Click](https://tianchi.aliyun.com/dataset/56)是阿里巴巴提供的一个淘宝展示广告点击率预估数据集。114万用户8天内的广告展示/点击日志（2600万条记录），用前面7天的做训练样本（20170506-20170512），用第8天的做测试样本（20170513）。
目前已有的研究：[CSDN](https://blog.csdn.net/weixin_39802763/article/details/105766042)，[CSDN](https://blog.csdn.net/sinat_28015305/article/details/108065830)，[arXiv](https://arxiv.org/abs/1706.0697)，[arXiv](https://arxiv.org/abs/1704.05194)。

1. **ad_feature.csv 广告信息表**，29.8MB

| **属性** | adgroup_id | cate_id | campaign_id | customer | brand | price |
| --- | --- | --- | --- | --- | --- | --- |
| **解释** | 广告ID | 商品类别ID | 广告计划ID | 广告主ID | 品牌ID | 商品价格 |
| **第一行数据** | 63133 | 6406 | 83237 | 1 | 95471 | 170.0 |

- 一个广告ID对应一个商品，一个商品属于一个类目，一个商品属于一个品牌。

2. **raw_sample.csv 原始样本骨架**，用户-广告展示/点击数据，1.01GB

| **属性** | user | time_stamp | adgroup_id | pid | nonclk | clk |
| --- | --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 日志时间戳 | 广告ID | 广告资源位 | 未点击 | 点击 |
| **第一行数据** | 581738 | 1494137644 | 1 | 430548_1007 | 1 | 0 |

- 未点击的时候，clk=0，nonclk=1

3. **user_profile.csv 用户信息表**，22.9MB

| **属性** | userid | cms_segid | cms_group_id | final_gender_code | age_level | pvalue_level | shopping_level | occupation | new_user_class_level |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 微群ID | 微群ID | 性别 | 年龄分层 | 消费能力 | 购物深度 | 职业 | 城市层级 |
| **第一行数据** | 234 | 0 | 5 | 2 | 5 |  | 3 | 0 | 3 |

- 性别：1-男，2-女
- 消费能力：1-低，2-中，3-高
- 购物深度：1-低，2-中，3-高
- 职业：是否是大学生，0-否，1-是

4. **behavior_log.csv 用户行为日志表**，22GB

| **属性** | user | time_stamp | btag | cate | brand |
| --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 日志时间戳 | 行为类型 | 商品类别ID | 品牌ID |
| **第一行数据** | 558157 | 1493741625 | pv | 6250 | 91286 |

- 行为标签：pv-浏览，cart-加入购物车，fav-喜欢，buy-购买

5. **基线AUC：0.622**


### 2.1.4 数据预处理与初步分析

#### 2.1.4.1 读取数据

源代码请查看[data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) 。

因为数据过大，使用采样读取`behavior_log.csv`，并保留采样的用户，主要代码如下：

```python
n_sample = int(frac * total_rows)  # 采样的行数
behavior_log = pd.read_csv(f'{root_path}/behavior_log.csv', nrows=n_sample)
sampled_users = behavior_log['user'].unique()  # 采样的用户
raw_sample = raw_sample[raw_sample['user'].isin(sampled_users)]  # 保留采样的用户
user_profile = user_profile[user_profile['userid'].isin(sampled_users)]
```

假设选取0.1%(即使0.1%也很大了)的`behavior_log.csv`数据，采样其他表格，最后得到的数据shape为：
```python
user_profile用户数据： (185915, 9)
raw_sample样本数据： (7114606, 6)
behavior_log用户行为数据： (723268, 5)
ad_feature广告特征数据： (846811, 6)
```

顺便，对同一含义的不同列名进行统一，统一为`user_id`和`cate_id`。


#### 2.1.4.2 缺失值&编码

`pvalue_level, new_user_class_level`、 `brand`有缺失值，比例分别为52.70%,26.45%、29.09%。在初步分析阶段暂时不做缺失值处理。
> new_user_class_level为分类属性，众数填充；brand为id类数据，填充上一条数据的值；pvalue_level通过KNN算法进行预测填充(train是pvalue_level≠0的行，test是pvalue_level=0的行。X是非pvalue_level属性，y是pvalue_level属性)。[来源](https://blog.csdn.net/weixin_39802763/article/details/105766042)

~~对`cate_id`, `brand`进行编码，使用`LabelEncoder`。意义不大，故不做了~~

#### 2.1.4.3 了解特征

源代码请看[data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) 。

下面列举部分特征的取值情况（分布较为均匀的、特征是ID的不列出，部分数据使用给出图像方便查看）：

1. **ad_feature**

基本为ID型数据，略去。

2. **raw_sample**

pid(2个取值)：

| 430548_1007 | 430539_1007 |
| --- | --- |
| 4016881 | 3097725 |

clk(2个取值)：

| 0 | 1 |
| --- | --- |
| 6722820 | 391786 |

样本不平衡率$\frac{6722820}{391786} = 17.16$，严重不平衡。$0$类占比94.5%。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/raw-sample-hour-distribution.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">用户在傍晚的数据反而较少，有可能是数据集做的时间脱敏，也可能是本身在傍晚的广告投放量较少，也可能是下班了陪伴家人或者刷视频而不会去摸鱼购物</p>
    </div>
</div>

3. **user_profile**

final_gender_code(2个取值)：

| 1 | 2 |
| --- | --- |
| 52258 | 133657 |

女性是男性的2.56倍，占71.9%。

pvalue_level消费能力(3个取值)：

| 1.0 | 2.0 | 3.0 |
| --- | --- | --- |
| 24351 | 55492 | 8467 |

大部分用户消费能力在中等水平与偏下水平。

shopping_level购物深度(3个取值)：

| 1 | 2 | 3 |
| --- | --- | --- |
| 7318 | 16606 | 161991 |

大部分用户购物深度在高水平。

occupation是否大学生(2个取值)：

| 0 | 1 |
| --- | --- |
| 174450 | 11465 |

大学生只占6.2%。

new_user_class_level城市层级(4个取值)：

| 1.0 | 2.0 | 3.0 | 4.0 |
| --- | --- | --- | --- |
| 16012 | 63962 | 33769 | 25057 |

大部分用户城市层级在中等水平与偏下水平。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/user_profile-distribution-1.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">分析见上文</p>
    </div>
</div>

4. **behavior_log**

btag(4个取值)：

| pv | cart | buy | fav |
| --- | --- | --- | --- |
| 688331 | 16119 | 9577 | 9241 |

加入购物车的比例是$\frac{16119}{688331} = 2.34\%$，购买的比例是$\frac{9577}{688331} = 1.39\%$。buy与fav相近，可能是用户购买之后加入了收藏，（yysy我才知道淘宝有收藏功能）具体的行为链还需要进一步分析。


day（20个取值，下面只列出最主要的3个）：

| 3 | 2 | 1 |
| --- | --- | --- |
| 641372 | 81573 | 190 |

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/day-hour-distribution-1.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">用户在傍晚的日志反而较少，这个与之前的图raw-sample-hour-distribution类似</p>
    </div>
</div>


#### 2.1.4.4 异常值等处理

让我们转回[data_preprocessing.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_1_data_preprocessing.ipynb) 。

通过2.1.4.3的图像输出发现ad_feature的price有极高的异常值(99999999.0)，采取`ad_feature.loc[ad_feature['price'] > 999999, 'price'] = 999999`处理。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/ad_feature-price-distribution-90percent.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">使用price_90 = np.percentile(ad_feature["price"], 90), sns.histplot(ad_feature["price"][ad_feature["price"] <= price_90], kde=False, bins=100)绘制前90%的数据</p>
    </div>
</div>

另外为了节约存储空间，这里顺便对pid进行编码。`raw_sample['pid'] = raw_sample['pid'].apply(lambda x: 1 if x == '430548_1007' else 2)`

#### 2.1.4.5 corr

回到[data_visualization.ipynb](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_2_data_visualization.ipynb) 。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/corr.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;"></p>
    </div>
</div>

后三个数据的corr较小，我们这里重点分析**用户画像**。
首先是`cms_segid`与`cms_group_id`与性别、年龄都有一定的相关性，可能是相似的用户往往会聚在一起，而这种聚集与购物深度、职业、城市层级等特征关系很小。
现在不考虑userid、cms_segid、cms_group_id这三列，`age_level`与`pvakue_level`相关性为0.25，与`occupation`相关性为-0.29，年龄越大消费能力越高，越不可能为大学生。`pvalue_level`与`occupation`和`new_user_class_level`相关性都是-0.11，说明消费能力越高越不可能为大学生，越可能是一线城市。


#### 2.1.4.6 合并数据

```python
# 1. 合并 raw_sample 和 user_profile。通过主键user_id 字段进行连接。
merged_data = pd.merge(raw_sample, user_profile, on='user_id', how='left')
# 2. 再将上述结果与 ad_feature 合并。通过主键adgroup_id 字段进行连接。
ad_u_data = pd.merge(merged_data, ad_feature, on='adgroup_id', how='left')
```
保存为`ad_u_data.csv`。
其实这等价于
```SQL
SELECT * FROM raw_sample
LEFT JOIN user_profile ON raw_sample.user_id = user_profile.userid
LEFT JOIN ad_feature ON raw_sample.adgroup_id = ad_feature.adgroup_id
```

在本次合并后，数据的shape为`(7114606, 21)`，存放在`G:/DataSets/Ali_Display_Ad_Click/processed_data/ad_u_data.csv`(654MB)里。其中包含了`raw_sample`、`user_profile`、`ad_feature`的所有信息。下面按照原先表格的顺序给出各个属性列的解释(方便查看以及喂给GPT)：

| **属性** | user_id | time_stamp | adgroup_id | pid | nonclk | clk | day | hour |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **解释** | 用户ID | 日志时间戳 | 广告ID | 广告资源位 | 未点击(未点击的时候，clk=0，nonclk=1) | 点击 | day | hour |

| **属性** | cms_segid | cms_group_id | final_gender_code | age_level | pvalue_level | shopping_level | occupation | new_user_class_level |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **解释** | 微群ID | 微群ID | 性别(1-男，2-女) | 年龄分层 | 消费能力(1-低，2-中，3-高) | 购物深度(1-低，2-中，3-高) | 职业(是否是大学生，0-否，1-是) | 城市层级 |

| **属性** | cate_id | campaign_id | customer | brand | price |
| --- | --- | --- | --- | --- | --- |
| **解释** | 商品类别ID | 广告计划ID | 广告主ID | 品牌ID | 商品价格 |

一个广告ID对应一个商品，一个商品属于一个类目，一个商品属于一个品牌。

#### 2.1.4.7 其余可视化

点击量/点击率的时间趋势：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/clk_nonclk_trend.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">点击与不点击的数量随时间的变化趋势，十分有规律</p>
    </div>
</div>


<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/clk_rate_trend.png" style="width: 60%;"/>
        <p style="font-size: small; color: gray;">点击率的变化不大</p>
    </div>
</div>

二八定律：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/28定律-brand.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">小于20%的品牌贡献了大于80%的广告数量</p>
    </div>
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/28定律-userclk.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">约20%的用户贡献了80%的点击量</p>
    </div>
</div>

性别、价格与点击率：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/price-density-by-gender.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">男性点击的商品价格一般比女性高，并且不点击的商品价格均值也都比点击的商品要高一点点</p>
    </div>
</div>


点击率与点击量的关系：

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/各点击数区间的广告点击率情况.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">x轴是点击的次数，点击量低的可能是某些小众群体的喜好，点击量高的可能是大众喜好，所以点击量高一点。因为平台的展示量比较固定，点击量与点击率近似成正比，所以我认为这个图体现的信息不多。</p>
    </div>
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/各展示量区间的广告点击率情况.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">x轴是展示的次数，展示次数多的点击率更高。这可能是与平台的正反馈的一个良性循环，质量越好、投放越精准的广告能让平台给予更大的支持力度，从而更加提升点击量与点击率</p>
    </div>
</div>


#### 2.1.4.8 构建embedding向量

代码可见[P_AD_3_book_optimbyGPT](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_3_book_optimbyGPT.py)，这是书上的代码经过GPT优化后的版本(文件名包含book的就是纯照抄书上的代码，加上byGPT后缀的就是经过GPT修改了的代码)。

这个代码依据id进行embedding来训练(使用点积结果与clk的BCE进行训练)，书上最后给了acc评价指标(我还加上了AUC指标)，但是我觉得这样评估不合适，如果**只是使用id**进行embedding，本身无法泛化，而valid的会被train的id泄露(请看中间的shape输出，我写了注释的)，不然怎么acc那么高呢？所以这个模型是不可靠的，只是一个embedding的示例，或者是只是为了建模每个id的embedding而已。

另外暂时无法使用pytorch复现结果(模型几乎不会去预测类别为1)，原因未知。

模型输出如下：

<div style="display: flex; justify-content: center; align-items: center;">
<div style=" max-height: 200px; max-width: 90%; overflow-y: auto; border: 1px solid #39c5bb; border-radius: 10px;">

```python
User length: 116430, Ad length: 76846
Input data: ((524761,), (524761,))   # 整个数据集，属性为user_id, adgroup_id
(419808, 2)  # 训练集的shape
(289265, 2)  # 验证集的user_id在训练集中出现的次数
(341164, 2)  # 验证集的adgroup_id在训练集中出现的次数
(227251, 2)  # 验证集的元组在训练集中出现的次数
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 adgroup_id (InputLayer)     [(None, 1)]                  0         []                            
                                                                                                  
 user_id (InputLayer)        [(None, 1)]                  0         []                            
                                                                                                  
 ad_embedding (Embedding)    (None, 1, 50)                3842300   ['adgroup_id[0][0]']          
                                                                                                  
 user_embedding (Embedding)  (None, 1, 50)                5821500   ['user_id[0][0]']             
                                                                                                  
 dot_product (Dot)           (None, 1, 1)                 0         ['ad_embedding[0][0]',        
                                                                     'user_embedding[0][0]']      
                                                                                                  
 reshape (Reshape)           (None, 1)                    0         ['dot_product[0][0]']         
                                                                                                  
 dense (Dense)               (None, 1)                    2         ['reshape[0][0]']             
                                                                                                  
==================================================================================================
Total params: 9663802 (36.86 MB)
Trainable params: 9663802 (36.86 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
开始训练 Embedding 模型
Epoch 1/3
257/257 [==============================] - 24s 93ms/step - loss: 0.6397 - accuracy: 0.8784 - val_loss: 0.5675 - val_accuracy: 0.9506
Epoch 2/3
257/257 [==============================] - 22s 87ms/step - loss: 0.5073 - accuracy: 0.9496 - val_loss: 0.4113 - val_accuracy: 0.9583
Epoch 3/3
257/257 [==============================] - 22s 86ms/step - loss: 0.3709 - accuracy: 0.9581 - val_loss: 0.2943 - val_accuracy: 0.9764
评估模型性能
3280/3280 [==============================] - 2s 641us/step
Predicted 0: 102252, Predicted 1: 2701
Predicted 0 ratio: 0.97, Predicted 1 ratio: 0.03
Validation AUC: 0.9998
              precision    recall  f1-score   support

           0       0.98      1.00      0.99     99772
           1       1.00      0.52      0.69      5181

    accuracy                           0.98    104953
   macro avg       0.99      0.76      0.84    104953
weighted avg       0.98      0.98      0.97    104953
```

</div>
</div>



### 2.1.5 特征工程

书里面的代码根本没办法运行，要么参数不对，要么OOM。对应的代码是P_AD_4_FE_book.py, P_AD_5_book_model.ipynb。

### 2.1.6 ItemCF

`N`记录每个广告被点击的总人数， `C`记录每对广告共同被点击的次数。
初始化共现矩阵的简要代码如下：
```python
for userid, item in dataSet.items():
    for ad_i, _ in item.items():
        N[ad_i] += 1  # 如果广告ad_i被用户点击，点击人数加1
        for ad_j, _ in item.items():
            if ad_j != ad_i:  # 不计算广告自身的共现
                C[ad_i][ad_j] += 1  # 如果用户同时点击了广告ad_i和广告ad_j，那么这两个广告的共现次数就加1
```

`W`是最终的相似度矩阵，`W[i][j] = C[i][j] / sqrt(N[i] * N[j])`：
```python
for ad_i, item in C.items():  # 遍历每个广告i
    for ad_j, count in item.items():  # 遍历广告i的共现广告j
        W[ad_i][ad_j] = C[ad_i][ad_j] / sqrt(N[ad_i] * N[ad_j])  # 余弦相似度
```

推荐方法有两种：
1. 基于物品的协同过滤：对于某个广告$i$，找到与广告$i$相似的广告$\mathcal{D}_j$(依据`W[i][j]`)，然后对$\mathcal{D}_j$按照相似度进行排序，推荐相似度最高的`top_n`个广告。
2. 基于用户的协同过滤：对于用户$u$，计算其历史上点击过的广告集合$\mathcal{D}_i$的相似广告$\mathcal{D}_j$，然后对$\mathcal{D}_j$按照相似度进行排序，推荐相似度最高的`top_n`个广告。

最终得到结果是：

```python
---与广告10000相似的广告---
             相似度
153247  1.000000
576560  1.000000
588978  1.000000
405680  0.707107
503010  0.707107
695765  0.707107
688732  0.577350
749768  0.577350
322827  0.577350
648820  0.577350
---为用户387456推荐的广告---
  adgroup_id      推荐分数
0     156293  0.577350
1     175825  0.577350
2      26738  0.534522
3     224929  0.377964
4     500288  0.377964
5     110597  0.377964
6     466129  0.377964
7     681901  0.377964
8      29126  0.377964
9     316317  0.377964
```


### 2.1.7 NCF

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/机器学习实践/images/image.png" style="width: 90%;"/>
        <p style="font-size: small; color: gray;">Neural Collaborative Filtering</p>
    </div>
</div>

这里主要实现[NCF](https://arxiv.org/pdf/1708.05031)中的Multi-Layer Perceptron (MLP)：
用户潜在向量User Latent Vector为$P \in \mathbb{R}^{M \times K}$，物品潜在向量Item Latent Vector为$Q \in \mathbb{R}^{N \times K}$，其中$M$是用户数，$N$是物品数，$K$是潜在向量的维度。然后将其拼接为$2K$维的向量，然后通过多层感知机(使用ReLU)，最后使用无bias的全连接层和激活函数得到预测分数。

论文中还有**GMF**(Generalized Matrix Factorization)、**NeuMF**(Neural Matrix Factorization)。**GMF**是MF的一种推广，使用element-wise的乘法(即哈达玛积，这里wise是way方法的意思)来模拟用户和物品之间的潜在特征交互。**NeuMF**将GMF和MLP的输出连接在一起，然后通过一个全连接层来预测评分。

参考资料：[Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031)、[知乎](https://zhuanlan.zhihu.com/p/158417209)。

作者的代码在[github](https://github.com/hexiangnan/neural_collaborative_filtering/tree/master)，还有其他人实现的[PyTorch版本](https://github.com/HeartbreakSurvivor/RsAlgorithms/blob/main/NCF/network.py)，我的代码在[github](https://github.com/virtualxiaoman/Easier_DataScience/blob/master/data_analyse/ML_HUB/Project_AD/P_AD_7_NCF_byGPT.py)。简化版本的模型代码如下，首先是嵌入：
```python
self.user_embedding = nn.Embedding(num_users, embedding_dim)
self.ad_embedding = nn.Embedding(num_ads, embedding_dim)
user_emb = self.user_embedding(user_id)
ad_emb = self.ad_embedding(adgroup_id)
```

GMF：
```python
self.fc = nn.Linear(embedding_dim, 1)
x = user_emb * ad_emb  # Element-wise multiplication
x = self.fc(x)
return torch.sigmoid(x).squeeze()
```

MLP：
```python
x = torch.cat([user_emb, ad_emb], dim=-1)  # 拼接用户和广告组的嵌入
x = self.relu(self.fc1(x))  # 全连接层，后面可以继续加dropout, fc2, fc3等层
  ...
return self.sigmoid(x).squeeze()  # Sigmoid 输出概率
```

NeuMF：
```python
self.fc = nn.Linear(mf_embedding_dim + mlp_hidden_dim, 1)
predict_vector = torch.cat([mf_output, mlp_output], dim=-1)  # Concatenate the outputs of GMF and MLP parts
prediction = torch.sigmoid(self.fc(predict_vector))  # Final prediction layer
```

注意，在我的实现里面使用带权重的`BCEWithLogitsLoss`，所以最后一层不需要`sigmoid`。如果不带权重，acc较高但是auc很低。如果加上权重，auc会高一些，结果如下：
```python
Epoch [100/100], Train Loss: 0.0786, Train Accuracy: 0.9816, Train AUC: 0.9916, Val Loss: 11.7236, Val Accuracy: 0.8234, Val AUC: 0.5455
Classification Report (Test):
              precision    recall  f1-score   support

         0.0       0.95      0.86      0.90     18894
         1.0       0.08      0.21      0.11      1106

    accuracy                           0.82     20000
   macro avg       0.51      0.53      0.51     20000
weighted avg       0.90      0.82      0.86     20000
```
上面的训练在epoch较大(30~40)的时候auc开始下降(train的上升，valid的下降)。

如果加上过采样，结果如下：
```python
Epoch [100/100], Train Loss: 0.0595, Train Accuracy: 0.9901, Train AUC: 0.9909, Val Loss: 9.9872, Val Accuracy: 0.4651, Val AUC: 0.5117
Classification Report (Test):
              precision    recall  f1-score   support

         0.0       0.95      0.46      0.62     18894
         1.0       0.06      0.55      0.10      1106

    accuracy                           0.47     20000
   macro avg       0.50      0.50      0.36     20000
weighted avg       0.90      0.47      0.59     20000
```

首先，二者都有问题：
1. 过拟合+类别不平衡，debuff叠满了。
2. 第二种方法使用`SMOTE(sampling_strategy='minority')`，过采样可能导致了模型对少数类的过度拟合，因为过采样的少数类样本可能与真实的用户行为不符(与现实数据分布不一致)。注意到`SMOTE`后，模型的R基本都是0.5了(随机猜测)，而且1的R更高一些，说明模型学到的是过采样的特征，而不是真实的特征。

修改学习率 `optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)`、dropout从0.2到0.5 `model = NCFModel(num_users=train_dataset.num_users, num_ads=train_dataset.num_ads, dropout=0.5).to(device)`参数后，结果如下：
```python
Epoch [100/100], Train Loss: 0.1323, Train Accuracy: 0.9754, Train AUC: 0.9799, Val Loss: 6.0587, Val Accuracy: 0.8890, Val AUC: 0.5630
Classification Report (Test):
              precision    recall  f1-score   support

         0.0       0.95      0.93      0.94     18894
         1.0       0.10      0.13      0.12      1106

    accuracy                           0.89     20000
   macro avg       0.53      0.53      0.53     20000
weighted avg       0.90      0.89      0.90     20000
```
如果在`epoch=30`处早停，结果几乎一致。
