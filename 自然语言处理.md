# $\color{ee0000} 1.\ 简介 $
## $\color{66ccff} 1.1\ 方法 $

- 符号主义：正则表达式、语言学
- 统计方法：N-gram、HMM、PCFG
- 联结主义：神经网络、深度学习

## $\color{66ccff} 1.2\ 参考资料 $
[动手学NLP](https://hnlp.boyuai.com/)：据说要出电子书，但是遥遥无期啊，最后还是买了纸质的。

# $\color{ee0000} 2.\ 文本规范化 $
## $\color{66ccff} 2.1\ 分词 $
### $\color{39c5bb} 2.1.1\ 英文 $

特点：词间有空格分隔。

1. **基于空格：**`tokens = sentence.split(' ')`

2. **基于正则表达式：**
  - **忽略所有标点：**`tokens = re.findall(r'\w+', sentence)`。（只替换标点而不分词：`re.sub(r'[^\w\s]', '', sentence)`）
  - **智能分词：**`\.\.\.|\$?\d+(?:\.\d+)?%?|(?:\w+\.)+\w+(?:\.)*|\w+(?:[-']\w+)*|\S\w*`
  其中：
  `\.\.\.`匹配省略号，
  `\$?\d+(?:\.\d+)?%?`匹配美元货币，
  `(?:\w+\.)+\w+(?:\.)*`匹配缩写或网址，
  `\w+(?:[-']\w+)*`匹配单词，
  `\S\w*`匹配连字符(比如It's的's或者单个标点符号)。
  最终效果是：
  ```python
  sentence = "Did you spend $3.4 on arxiv.org for your pre-print?   No, it's free! It's ..."
  pattern = r"\.\.\.|\$?\d+(?:\.\d+)?%?|(?:\w+\.)+\w+(?:\.)*|\w+(?:[-']\w+)*|\S\w*"
  print(re.findall(pattern, sentence))
  # 输出：['Did', 'you', 'spend', '$3.4', 'on', 'arxiv.org', 'for', 'your', 'pre-print', '?', 'No', ',', "it's", 'free', '!', "It's", '...']
  ```
  注：`(?:...)`是非捕获组，也就是匹配的时候要匹配到这个，但是不把这个组的内容放到匹配结果里面，比如`re.findall(r'(?:\w+)-(\d+)', 'abc-123 def-456')`的结果是`['123', '456']`。

<div align=center>
<img src="assets/自然语言处理/images/image-1.png" width="80%">
</div>

### $\color{39c5bb} 2.1.2\ 中文 $

特点：词间无空格分隔。

1. **基于监督学习的序列标注模型：** CRF
2. **基于子词的分词：** 字节对编码BPE、一元语言建模分词unigram language modeling tokenization、词片WordPiece。

<p style="color:#EC407A; font-weight:bold">BPE</p>

1. **初始化：** 将所有字符作为词并在最后面加上`_`，然后统计每个前后相连字符对的频率。比如将词`beijing`拆分成字符`['b', 'e', 'i', 'j', 'i', 'n', 'g', '_']`，统计语料库里所有的词`'be', 'ei', 'ij', 'ji', 'in', 'ng', 'g_'`的频率。
2. **合并：** 合并频率最高的字符对`merge_key`，更新字符对的频率。假设`merge_key`是`ng`，那么之前的`'n', 'g'`会被替换为`'ng'`。
3. **重复：** 重复步骤2，直到达到指定的词表大小或达到指定迭代次数。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>

代码实现：

```python
# 1. 语料库
corpus = "nan nan nan nan nan nanjing nanjing beijing beijing beijing beijing beijing beijing dongbei dongbei dongbei bei bei"
tokens = corpus.split(' ')

# 构建基于字符的初始词表
vocabulary = set(corpus)  # {'a', ' ', 'j', 'd', 'o', 'e', 'g', 'b', 'n', 'i'}
vocabulary.remove(' ')
vocabulary.add('_')
vocabulary = sorted(list(vocabulary))
print(vocabulary)  # ['_', 'a', 'b', 'd', 'e', 'g', 'i', 'j', 'n', 'o']

# 根据语料构建词表
corpus_dict = {}
for token in tokens:
    key = token + '_'
    if key not in corpus_dict:
        corpus_dict[key] = {"split": list(key), "count": 0}
    corpus_dict[key]['count'] += 1  # 比如：'nan_': {'split': ['n', 'a', 'n', '_'], 'count': 5}

print(f"语料：")
for key in corpus_dict:
    print(corpus_dict[key]['count'], corpus_dict[key]['split'])
print(f"词表：{vocabulary}")

# 2. BPE词元学习器
for step in range(9):
    print(f"------第{step + 1}次迭代------")
    split_dict = {}  # 用于统计符号组合的出现次数
    for key in corpus_dict:
        splits = corpus_dict[key]['split']  # key是当前符号nan_，splits是分割后的符号['n', 'a', 'n', '_']
        print(f"当前符号：{key}, 分割后的符号：{splits}")
        # 遍历所有符号进行统计
        for i in range(len(splits) - 1):
            # 组合两个符号作为新的符号
            current_group = splits[i] + splits[i + 1]
            if current_group not in split_dict:
                split_dict[current_group] = 0
            split_dict[current_group] += corpus_dict[key]['count']

    group_hist = [(k, v) for k, v in sorted(split_dict.items(), key=lambda item: item[1], reverse=True)]
    print(f"当前最常出现的前5个符号组合：{group_hist[:5]}")

    merge_key = group_hist[0][0]
    print(f"本次迭代组合的符号为：{merge_key}")
    for key in corpus_dict:
        if merge_key in key:
            new_splits = []
            splits = corpus_dict[key]['split']
            i = 0
            while i < len(splits):
                if i + 1 >= len(splits):
                    new_splits.append(splits[i])
                    i += 1
                    continue
                if merge_key == splits[i] + splits[i + 1]:
                    new_splits.append(merge_key)
                    i += 2
                else:
                    new_splits.append(splits[i])
                    i += 1
            corpus_dict[key]['split'] = new_splits  # 更新分割后的符号，比如merge_key是'ng'，那么之前的'n', 'g'会被替换为'ng'

    vocabulary.append(merge_key)
    print(f"迭代后的语料为：")
    for key in corpus_dict:
        print(corpus_dict[key]['count'], corpus_dict[key]['split'])
    print(f"词表：{vocabulary}")
ordered_vocabulary = {key: i for i, key in enumerate(vocabulary)}

# 3. BPE词元分词器
sentence = "nanjing beijing"
print(f"--------输入语句：{sentence}--------")
tokens = sentence.split(' ')
tokenized_string = []
for token in tokens:
    key = token + '_'
    splits = list(key)
    # 用于在没有更新的时候跳出
    flag = 1
    while flag:
        flag = 0
        split_dict = {}
        # 遍历所有符号进行统计
        for i in range(len(splits) - 1):
            # 组合两个符号作为新的符号
            current_group = splits[i] + splits[i + 1]
            if current_group not in ordered_vocabulary:
                continue  # 如果当前组合不在词表里，跳过
            if current_group not in split_dict:
                # 判断当前组合是否在词表里，如果是的话加入split_dict
                split_dict[current_group] = ordered_vocabulary[current_group]
                flag = 1
        if not flag:
            continue
        print(f"当前分词：{splits}")
        print(f"当前组合：{split_dict}")
        # 对每个组合进行优先级的排序（此处为从小到大）
        group_hist = [(k, v) for k, v in sorted(split_dict.items(), key=lambda item: item[1])]
        # 优先级最高的组合
        merge_key = group_hist[0][0]
        print(f"当前优先级最高的{merge_key}")
        new_splits = []
        i = 0
        # 根据优先级最高的组合产生新的分词
        while i < len(splits):
            if i + 1 >= len(splits):
                new_splits.append(splits[i])
                i += 1
                continue
            if merge_key == splits[i] + splits[i + 1]:
                new_splits.append(merge_key)
                i += 2
            else:
                new_splits.append(splits[i])
                i += 1
        splits = new_splits
    tokenized_string += splits

print(f"分词结果：{tokenized_string}")
```

</details>


## $\color{66ccff} 2.2\ 词规范化 $
### $\color{39c5bb} 2.2.1\ 英文 $

1. **大小写转换：** `sentence.lower()`
2. **词目还原：** `nltk.WordNetLemmatizer().lemmatize(word, pos='v')`，其中`pos`是词性，比如`v`是动词。
3. **词干还原：** `nltk.PorterStemmer().stem(word)`

### $\color{39c5bb} 2.2.2\ 中文 $

1. **繁简转换：** `opencc`、`hanziconv`


## $\color{66ccff} 2.3\ 分句 $
### $\color{39c5bb} 2.3.1\ 英文 $

1. **基于正则表达式：** `re.split(r'[.!?]', text)`，该方法可能会出现歧义，标点`.!?`可能会出现在缩写、数字、网址其他等地方。正确做法是先分词，再分句。如下：

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>  

```python
import re
sentence_spliter = {".", "?", '!', '...'}
sentence = "Did you spend $3.4 on arxiv.org for your pre-print? No, it's free! It's ..."
pattern = r"\.\.\.|\$?\d+(?:\.\d+)?%?|(?:\w+\.)+\w+(?:\.)*|\w+(?:[-']\w+)*|\S\w*"
tokens = re.findall(pattern, sentence)
# ['Did', 'you', 'spend', '$3.4', 'on', 'arxiv.org', 'for', 'your', 'pre-print', '?', 'No', ',', "it's", 'free', '!', "It's", '...']
print(tokens)
sentences = []
boundary = [0]
# 遍历所有token，如果该token是句子边界，则将句子加入sentences
for token_id, token in enumerate(tokens):
    if token in sentence_spliter:
        # 如果是句子边界，则把分句结果加入进去
        sentences.append(tokens[boundary[-1]:token_id + 1])
        # 将下一句句子起始位置加入boundary
        boundary.append(token_id + 1)
    # print(sentences)
# 即使最后一个句子不是句子边界，也要加入进去
if boundary[-1] != len(tokens):
    sentences.append(tokens[boundary[-1]:])

print(f"分句结果：")
for seg_sentence in sentences:
    print(seg_sentence)
```
输出：
```
['Did', 'you', 'spend', '$3.4', 'on', 'arxiv.org', 'for', 'your', 'pre-print', '?']
['No', ',', "it's", 'free', '!']
["It's", '...']
```
</details>

### $\color{39c5bb} 2.3.2\ 中文 $

1. **基于正则表达式：** `re.split(r'[。！？]', text)`，同样可能会出现歧义，正确做法是先分词，再分句。

# $\color{ee0000} 3.\ 文本表示 $

## $\color{66ccff} 3.1\ 词向量 $

这里只给出GloVe的使用，不具体讨论词向量应该怎么表示更好。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>

```python
import pprint  # 美化输出
from gensim.models import KeyedVectors  # 加载词向量

model = KeyedVectors.load_word2vec_format('../model/official/glove/glove.6B.100d.txt', binary=False, no_header=True)
# 使用most_similar()找到词表中距离给定词最近（最相似）的n个词
pprint.pprint(model.most_similar('film'))
pprint.pprint(model.most_similar('car'))

# 类比
def analogy(x1, x2, y1):
    # y1 + x2 - x1
    result = model.most_similar(positive=[y1, x2], negative=[x1])
    return result[0][0]
print(analogy('china', 'chinese', 'japan'))  # japanese
print(analogy('australia', 'koala', 'china'))  # panda
print(analogy('tall', 'tallest', 'long'))  # longest
print(analogy('good', 'fantastic', 'bad'))  # terrible
print(analogy('man', 'woman', 'king'))  # queen
```

</details>

## $\color{66ccff} 3.2\ 稀疏向量 $

1. **共现矩阵：**`cooccur_matrix`大小为(vocab_size, vocab_size)，`cooccur_matrix[i, j]`表示词`i`和词`j`共现(在同一个窗口内，窗口长度可以设为`5`)的次数，是对称矩阵，对角线元素无效(实际过程中用0填充)。即$C_{ij} = \sum_{t=1}^{T} \mathbb{I}(w_t = i) \sum_{-c \leq j \leq c, j \neq 0} \mathbb{I}(w_{t+j} = j)$。与推荐系统当时问题一样，高频词会与很多词共现，主导着余弦相似度的计算结果。
2. **PPMI矩阵：**`ppmi_matrix`，大小为(vocab_size, vocab_size)，
`ppmi_matrix[i, j]`表示词`i`和词`j`的正点互信息(pointwise mutual information)值，是对称矩阵。即$\text {PMI}(i, j) = \log \frac{P(i, j)}{P(i)P(j)}$，$\text {PPMI}_{ij} = \max(\text {PMI}(i, j), 0)$。式中，$P(i, j) = \frac{C_{ij}}{N},P(i)=\frac{\sum_jC_{ij}}{N}$，$N$是语料库中的词数。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>

代码来自：https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus

```python
def co_occurrence(sentences, window_size):
    d = defaultdict(int)
    vocab = set()
    for text in sentences:
        # preprocessing (use tokenizer instead)
        text = text.lower().split()
        # iterate over sentences
        for i in range(len(text)):
            token = text[i]
            vocab.add(token)  # add to vocab
            next_token = text[i+1 : i+1+window_size]
            for t in next_token:
                key = tuple( sorted([t, token]) )
                d[key] += 1
    
    # formulate the dictionary into dataframe
    vocab = sorted(vocab) # sort vocab
    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),
                      index=vocab,
                      columns=vocab)
    for key, value in d.items():
        df.at[key[0], key[1]] = value
        df.at[key[1], key[0]] = value
    return df

def pmi(df, positive=True):
    col_totals = df.sum(axis=0)
    total = col_totals.sum()
    row_totals = df.sum(axis=1)
    expected = np.outer(row_totals, col_totals) / total
    df = df / expected
    # Silence distracting warnings about log(0):
    with np.errstate(divide='ignore'):
        df = np.log(df)
    df[np.isinf(df)] = 0.0  # log(0) = 0
    if positive:
        df[df < 0] = 0.0
    return df

text = ["I go to school every day by bus .",
        "i go to theatre every night by bus"] 
df = co_occurrence(text, 2)
ppmi = pmi(df, positive=True)
```

</details>

## $\color{66ccff} 3.3\ 稠密向量 $

1. **基于SVD的潜在语义分析LSA**：对于一个文档-词语矩阵$A$(term-document matrix)，每行代表一个词，每列代表一个文档，矩阵中的值表示词在文档中的出现频率。$A = UΣV^T$取前$k$个特征值，左奇异矩阵$U$表示词语的特征向量，右奇异矩阵$V^T$表示文档的特征向量。

2. **word2vec**：在[推荐系统2.2.1](推荐系统.html)里，这里只给出pytorch的实现。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>

```python
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import defaultdict
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from tqdm import trange
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from torch.optim import Adam

# 使用类管理数据对象，包括文本读取、文本预处理等
class TheLittlePrinceDataset:
    def __init__(self, tokenize=True):
        # 利用NLTK函数进行分句和分词
        text = open('data/the little prince.txt', 'r', encoding='utf-8').read()
        if tokenize:
            self.sentences = sent_tokenize(text.lower())
            self.tokens = [word_tokenize(sent) for sent in self.sentences]
        else:
            self.text = text

    def build_vocab(self, min_freq=1):
        # 统计词频
        frequency = defaultdict(int)
        for sentence in self.tokens:
            for token in sentence:
                frequency[token] += 1
        self.frequency = frequency

        # 加入<unk>处理未登录词，加入<pad>用于对齐变长输入进而加速
        self.token2id = {'<unk>': 1, '<pad>': 0}
        self.id2token = {1: '<unk>', 0: '<pad>'}
        for token, freq in sorted(frequency.items(), key=lambda x: -x[1]):
            # 丢弃低频词
            if freq > min_freq:
                self.token2id[token] = len(self.token2id)
                self.id2token[len(self.id2token)] = token
            else:
                break

    def get_word_distribution(self):
        distribution = np.zeros(vocab_size)
        for token, freq in self.frequency.items():
            if token in dataset.token2id:
                distribution[dataset.token2id[token]] = freq
            else:
                # 不在词表中的词按<unk>计算
                distribution[1] += freq
        distribution /= distribution.sum()
        return distribution

    # 将分词结果转化为索引表示
    def convert_tokens_to_ids(self, drop_single_word=True):
        self.token_ids = []
        for sentence in self.tokens:
            token_ids = [self.token2id.get(token, 1) for token in sentence]
            # 忽略只有一个token的序列，无法计算loss
            if len(token_ids) == 1 and drop_single_word:
                continue
            self.token_ids.append(token_ids)

        return self.token_ids

dataset = TheLittlePrinceDataset()
dataset.build_vocab(min_freq=1)
sentences = dataset.convert_tokens_to_ids()

# 遍历所有的中心词-上下文词对
window_size = 2
data = []  # 存储中心词-上下文词对
for sentence in sentences:
    for i in range(len(sentence)):
        for j in range(i-window_size, i+window_size+1):
            if j == i or j < 0 or j >= len(sentence):
                continue
            center_word = sentence[i]
            context_word = sentence[j]
            data.append([center_word, context_word])

data = np.array(data)
print(data.shape, data)

# 实现skipgram算法，使用对比学习计算损失
class SkipGramNCE(nn.Module):
    def __init__(self, vocab_size, embed_size, distribution, neg_samples=20):
        super(SkipGramNCE, self).__init__()
        print(f'vocab_size = {vocab_size}, embed_size = {embed_size}, ' + f'neg_samples = {neg_samples}')
        self.input_embeddings = nn.Embedding(vocab_size, embed_size)
        self.output_embeddings = nn.Embedding(vocab_size, embed_size)
        distribution = np.power(distribution, 0.75)
        distribution /= distribution.sum()
        self.distribution = torch.tensor(distribution)
        self.neg_samples = neg_samples

    def forward(self, input_ids, labels):
        i_embed = self.input_embeddings(input_ids)
        o_embed = self.output_embeddings(labels)
        batch_size = i_embed.size(0)
        n_words = torch.multinomial(self.distribution, batch_size * self.neg_samples, replacement=True).view(batch_size, -1)
        n_embed = self.output_embeddings(n_words)
        pos_term = F.logsigmoid(torch.sum(i_embed * o_embed, dim=1))
        # 负采样，用于对比学习
        neg_term = F.logsigmoid(- torch.bmm(n_embed, i_embed.unsqueeze(2)).squeeze())
        neg_term = torch.sum(neg_term, dim=1)
        loss = - torch.mean(pos_term + neg_term)
        return loss


# 为对比学习负采样准备词频率分布
vocab_size = len(dataset.token2id)
embed_size = 128
distribution = dataset.get_word_distribution()
print(distribution)
model = SkipGramNCE(vocab_size, embed_size, distribution)


# 定义静态方法collate_batch批量处理数据，转化为PyTorch可以需要的张量类型
class DataCollator:
    @classmethod
    def collate_batch(cls, batch):
        batch = np.array(batch)
        input_ids = torch.tensor(batch[:, 0], dtype=torch.long)
        labels = torch.tensor(batch[:, 1], dtype=torch.long)
        return {'input_ids': input_ids, 'labels': labels}


# 定义训练参数以及训练循环
epochs = 100
batch_size = 128
learning_rate = 1e-3
epoch_loss = []

data_collator = DataCollator()
dataloader = DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=data_collator.collate_batch)
optimizer = Adam(model.parameters(), lr=learning_rate)
model.zero_grad()
model.train()

# 训练过程，每步读取数据，送入模型计算损失，并使用PyTorch进行优化
with trange(epochs, desc='epoch', ncols=60) as pbar:
    for epoch in pbar:
        for step, batch in enumerate(dataloader):
            loss = model(**batch)
            pbar.set_description(f'epoch-{epoch}, loss={loss.item():.4f}')
            loss.backward()
            optimizer.step()
            model.zero_grad()
        epoch_loss.append(loss.item())

epoch_loss = np.array(epoch_loss)
plt.plot(range(len(epoch_loss)), epoch_loss)
plt.xlabel('training epoch')
plt.ylabel('loss')
plt.show()
# 保存模型
torch.save(model, '../model/prince/skipgram_nce.pth')
# 保存词向量
embeddings = model.input_embeddings.weight.data.numpy()
np.save('../model/prince/embeddings.npy', embeddings)
# 查询happy和sad的词向量，然后计算它们的余弦相似度
happy = embeddings[dataset.token2id['happy']]
sad = embeddings[dataset.token2id['sad']]
similarity = np.dot(happy, sad) / (np.linalg.norm(happy) * np.linalg.norm(sad))
print(similarity)  # 0.05455044
```

</details>


## $\color{66ccff} 3.4\ 文档表示 $

1. **TF-IDF**：
**TF**: Term Frequency，词频，表示词在文档中出现的频率。
**IDF**: Inverse Document Frequency，逆文档频率，表示词在文档集合中的稀有程度。
$$\text {TF}(w, d) = \frac{f_{w,d}}{\sum_{w' \in d} f_{w',d}}, \text {IDF}(w) = \log \frac{N}{\text {df}_w(+1)}$$
其中$f_{w,d}$是词$w$在文档$d$中的频率，$\text {df}_w$是包含词$w$的文档数，$N$是文档总数，$+1$是为了避免分母为0。

<details>
<summary><span style="color:#009688; font-weight:bold">点击展开完整代码</span></summary>

```python
class TFIDF:
    def __init__(self, vocab_size, norm='l2', smooth_idf=True, sublinear_tf=True):
        self.vocab_size = vocab_size
        self.norm = norm  # l2能让不同文档的TF-IDF向量长度一致
        self.smooth_idf = smooth_idf
        self.sublinear_tf = sublinear_tf

    def fit(self, X):
        doc_freq = np.zeros(self.vocab_size, dtype=np.float64)
        for data in X:
            for token_id in set(data):
                doc_freq[token_id] += 1
        doc_freq += int(self.smooth_idf)
        n_samples = len(X) + int(self.smooth_idf)
        self.idf = np.log(n_samples / doc_freq) + 1

    def transform(self, X):
        assert hasattr(self, 'idf')
        term_freq = np.zeros((len(X), self.vocab_size), dtype=np.float64)
        for i, data in enumerate(X):
            for token in data:
                term_freq[i, token] += 1
        if self.sublinear_tf:
            term_freq = np.log(term_freq + 1)
        Y = term_freq * self.idf
        if self.norm:
            row_norm = (Y ** 2).sum(axis=1)
            row_norm[row_norm == 0] = 1
            Y /= np.sqrt(row_norm)[:, None]
        return Y

    def fit_transform(self, X):
        self.fit(X)
        return self.transform(X)
```
</details>


2. **稠密向量**：
- 将文档里所有词的embedding求平均(平均池化、最大池化、注意力池化等)。
- 使用RNN、Transformer等模型的最后一层输出。